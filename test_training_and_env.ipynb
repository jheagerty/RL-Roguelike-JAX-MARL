{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [--help] [--hydra-help] [--version]\n",
      "                             [--cfg {job,hydra,all}] [--resolve]\n",
      "                             [--package PACKAGE] [--run] [--multirun]\n",
      "                             [--shell-completion] [--config-path CONFIG_PATH]\n",
      "                             [--config-name CONFIG_NAME]\n",
      "                             [--config-dir CONFIG_DIR]\n",
      "                             [--experimental-rerun EXPERIMENTAL_RERUN]\n",
      "                             [--info [{all,config,defaults,defaults-tree,plugins,searchpath}]]\n",
      "                             [overrides ...]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/home/jvnheagerty/.local/share/jupyter/runtime/kernel-v3c73c4712927c9f9b0fcd4b80823aa1946aed70fb.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/jvnhe/Projects/RL-Roguelike-JAX-MARL/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3558: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#development.ipynb\n",
    "import environment_MARL\n",
    "import data_classes\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "from flax.linen.initializers import constant, orthogonal\n",
    "from typing import Sequence, NamedTuple, Any, Tuple, Union, Dict\n",
    "import distrax\n",
    "\n",
    "\n",
    "import functools\n",
    "\n",
    "class ScannedRNN(nn.Module):\n",
    "    @functools.partial(\n",
    "        nn.scan,\n",
    "        variable_broadcast=\"params\",\n",
    "        in_axes=0,\n",
    "        out_axes=0,\n",
    "        split_rngs={\"params\": False},\n",
    "    )\n",
    "    @nn.compact\n",
    "    def __call__(self, carry, x):\n",
    "        \"\"\"Applies the module.\"\"\"\n",
    "        rnn_state = carry\n",
    "        ins, resets = x\n",
    "        rnn_state = jnp.where(\n",
    "            resets[:, np.newaxis],\n",
    "            self.initialize_carry(ins.shape[0], ins.shape[1]),\n",
    "            rnn_state,\n",
    "        )\n",
    "        new_rnn_state, y = nn.GRUCell(features=ins.shape[1])(rnn_state, ins)\n",
    "        return new_rnn_state, y\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_carry(batch_size, hidden_size):\n",
    "        # Use a dummy key since the default state init fn is just zeros.\n",
    "        cell = nn.GRUCell(features=hidden_size)\n",
    "        return cell.initialize_carry(jax.random.PRNGKey(0), (batch_size, hidden_size))\n",
    "\n",
    "\n",
    "class ActorRNN(nn.Module):\n",
    "    action_dim: Sequence[int]\n",
    "    config: Dict\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, hidden, x):\n",
    "        obs, dones, avail_actions = x\n",
    "        embedding = nn.Dense(\n",
    "            128, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(obs)\n",
    "        embedding = nn.relu(embedding)\n",
    "\n",
    "        rnn_in = (embedding, dones)\n",
    "        hidden, embedding = ScannedRNN()(hidden, rnn_in)\n",
    "\n",
    "        actor_mean = nn.Dense(128, kernel_init=orthogonal(2), bias_init=constant(0.0))(\n",
    "            embedding\n",
    "        )\n",
    "        actor_mean = nn.relu(actor_mean)\n",
    "        action_logits = nn.Dense(\n",
    "            self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        unavail_actions = 1 - avail_actions\n",
    "        action_logits = action_logits - (unavail_actions * 1e10)\n",
    "\n",
    "        pi = distrax.Categorical(logits=action_logits)\n",
    "\n",
    "        return hidden, pi\n",
    "\n",
    "\n",
    "class CriticRNN(nn.Module):\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, hidden, x):\n",
    "        world_state, dones = x\n",
    "        embedding = nn.Dense(\n",
    "            128, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(world_state)\n",
    "        embedding = nn.relu(embedding)\n",
    "        \n",
    "        rnn_in = (embedding, dones)\n",
    "        hidden, embedding = ScannedRNN()(hidden, rnn_in)\n",
    "        \n",
    "        critic = nn.Dense(128, kernel_init=orthogonal(2), bias_init=constant(0.0))(\n",
    "            embedding\n",
    "        )\n",
    "        critic = nn.relu(critic)\n",
    "        critic = nn.Dense(1, kernel_init=orthogonal(1.0), bias_init=constant(0.0))(\n",
    "            critic\n",
    "        )\n",
    "        \n",
    "        return hidden, jnp.squeeze(critic, axis=-1)\n",
    "    \n",
    "from jaxmarl.wrappers.baselines import JaxMARLWrapper\n",
    "from functools import partial\n",
    "\n",
    "class HanabiWorldStateWrapper(JaxMARLWrapper):\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def reset(self,\n",
    "              key):\n",
    "        obs, env_state = self._env.reset(key)\n",
    "        obs[\"world_state\"] = self.world_state(obs, env_state)\n",
    "        return obs, env_state\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def step(self,\n",
    "             key,\n",
    "             state,\n",
    "             action):\n",
    "        obs, env_state, reward, done, info = self._env.step(\n",
    "            key, state, action\n",
    "        )\n",
    "        obs[\"world_state\"] = self.world_state(obs, state)\n",
    "        return obs, env_state, reward, done, info\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def world_state(self, obs, state):\n",
    "        \"\"\" \n",
    "        For each agent: [agent obs, own hand]\n",
    "        \"\"\"\n",
    "            \n",
    "        all_obs = jnp.array([obs[agent] for agent in self._env.agents])\n",
    "        # hands = state.player_hands.reshape((self._env.num_agents, -1))\n",
    "        return all_obs\n",
    "        \n",
    "    \n",
    "    def world_state_size(self):\n",
    "   \n",
    "        return data_classes.get_observation_size(data_classes.schema) # NOTE hardcoded hand size\n",
    "    \n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "from typing import Sequence, NamedTuple, Any, Tuple, Union, Dict\n",
    "\n",
    "from flax.training.train_state import TrainState\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import jaxmarl\n",
    "from jaxmarl.wrappers.baselines import LogWrapper\n",
    "\n",
    "import wandb\n",
    "from config import train_config\n",
    "\n",
    "class Transition(NamedTuple):\n",
    "    global_done: jnp.ndarray\n",
    "    done: jnp.ndarray\n",
    "    action: jnp.ndarray\n",
    "    value: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    log_prob: jnp.ndarray\n",
    "    obs: jnp.ndarray\n",
    "    world_state: jnp.ndarray\n",
    "    info: jnp.ndarray\n",
    "    avail_actions: jnp.ndarray\n",
    "\n",
    "\n",
    "def batchify(x: dict, agent_list, num_actors):\n",
    "    x = jnp.stack([x[a] for a in agent_list])\n",
    "    return x.reshape((num_actors, -1))\n",
    "\n",
    "\n",
    "def unbatchify(x: jnp.ndarray, agent_list, num_envs, num_actors):\n",
    "    x = x.reshape((num_actors, num_envs, -1))\n",
    "    return {a: x[i] for i, a in enumerate(agent_list)}\n",
    "\n",
    "\n",
    "def make_train(config):\n",
    "    env = environment_MARL.RL_Roguelike_JAX_MARL()\n",
    "    num_actions = env.num_moves  # Get actual number of actions from environment\n",
    "    config[\"NUM_ACTORS\"] = env.num_agents * config[\"NUM_ENVS\"]\n",
    "    config[\"NUM_UPDATES\"] = (\n",
    "        config[\"TOTAL_TIMESTEPS\"] // config[\"NUM_STEPS\"] // config[\"NUM_ENVS\"]\n",
    "    )\n",
    "    config[\"MINIBATCH_SIZE\"] = (\n",
    "        config[\"NUM_ACTORS\"] * config[\"NUM_STEPS\"] // config[\"NUM_MINIBATCHES\"]\n",
    "    )\n",
    "    config[\"CLIP_EPS\"] = config[\"CLIP_EPS\"] / env.num_agents if config[\"SCALE_CLIP_EPS\"] else config[\"CLIP_EPS\"]\n",
    "\n",
    "    # env = FlattenObservationWrapper(env) # NOTE need a batchify wrapper\n",
    "    env = HanabiWorldStateWrapper(env)\n",
    "    env = LogWrapper(env)\n",
    "\n",
    "    def linear_schedule(count):\n",
    "        frac = (\n",
    "            1.0\n",
    "            - (count // (config[\"NUM_MINIBATCHES\"] * config[\"UPDATE_EPOCHS\"]))\n",
    "            / config[\"NUM_UPDATES\"]\n",
    "        )\n",
    "        return config[\"LR\"] * frac\n",
    "\n",
    "    def train(rng):\n",
    "        # INIT NETWORK\n",
    "        actor_network = ActorRNN(num_actions, config=config)\n",
    "        critic_network = CriticRNN()\n",
    "        rng, _rng_actor, _rng_critic = jax.random.split(rng, 3)\n",
    "        ac_init_x = (\n",
    "            jnp.zeros((1, config[\"NUM_ENVS\"], data_classes.get_observation_size(data_classes.schema))),\n",
    "            jnp.zeros((1, config[\"NUM_ENVS\"])),\n",
    "            jnp.zeros((1, config[\"NUM_ENVS\"], num_actions)),\n",
    "        )\n",
    "        ac_init_hstate = ScannedRNN.initialize_carry(config[\"NUM_ENVS\"], 128)\n",
    "        actor_network_params = actor_network.init(_rng_actor, ac_init_hstate, ac_init_x)\n",
    "        \n",
    "        cr_init_x = (\n",
    "            jnp.zeros((1, config[\"NUM_ENVS\"], env.world_state_size(),)), \n",
    "            jnp.zeros((1, config[\"NUM_ENVS\"])),\n",
    "        )\n",
    "        cr_init_hstate = ScannedRNN.initialize_carry(config[\"NUM_ENVS\"], 128)\n",
    "        critic_network_params = critic_network.init(_rng_critic, cr_init_hstate, cr_init_x)\n",
    "        \n",
    "        if config[\"ANNEAL_LR\"]:\n",
    "            actor_tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(learning_rate=linear_schedule, eps=1e-5),\n",
    "            )\n",
    "            critic_tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(learning_rate=linear_schedule, eps=1e-5),\n",
    "            )\n",
    "        else:\n",
    "            actor_tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(config[\"LR\"], eps=1e-5),\n",
    "            )\n",
    "            critic_tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(config[\"LR\"], eps=1e-5),\n",
    "            )\n",
    "        actor_train_state = TrainState.create(\n",
    "            apply_fn=actor_network.apply,\n",
    "            params=actor_network_params,\n",
    "            tx=actor_tx,\n",
    "        )\n",
    "        critic_train_state = TrainState.create(\n",
    "            apply_fn=actor_network.apply,\n",
    "            params=critic_network_params,\n",
    "            tx=critic_tx,\n",
    "        )\n",
    "\n",
    "        # INIT ENV\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        reset_rng = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "        obsv, env_state = jax.vmap(env.reset, in_axes=(0,))(reset_rng)\n",
    "        ac_init_hstate = ScannedRNN.initialize_carry(config[\"NUM_ACTORS\"], 128)\n",
    "        cr_init_hstate = ScannedRNN.initialize_carry(config[\"NUM_ACTORS\"], 128)\n",
    "\n",
    "        # TRAIN LOOP\n",
    "        def _update_step(update_runner_state, unused):\n",
    "            # COLLECT TRAJECTORIES\n",
    "            runner_state, update_steps = update_runner_state\n",
    "            \n",
    "            def _env_step(runner_state, unused):\n",
    "                train_states, env_state, last_obs, last_done, hstates, rng = runner_state\n",
    "\n",
    "                # SELECT ACTION\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                avail_actions = jax.vmap(env.get_legal_moves)(env_state.env_state)\n",
    "                avail_actions = jax.lax.stop_gradient(\n",
    "                    batchify(avail_actions, env.agents, config[\"NUM_ACTORS\"])\n",
    "                )\n",
    "                obs_batch = batchify(last_obs, env.agents, config[\"NUM_ACTORS\"])\n",
    "                ac_in = (\n",
    "                    obs_batch[np.newaxis, :],\n",
    "                    last_done[np.newaxis, :],\n",
    "                    avail_actions,\n",
    "                )\n",
    "                ac_hstate, pi = actor_network.apply(train_states[0].params, hstates[0], ac_in)\n",
    "                action = pi.sample(seed=_rng)\n",
    "                log_prob = pi.log_prob(action)\n",
    "                env_act = unbatchify(\n",
    "                    action, env.agents, config[\"NUM_ENVS\"], env.num_agents\n",
    "                )\n",
    "\n",
    "                # VALUE\n",
    "                world_state = last_obs[\"world_state\"].reshape((config[\"NUM_ACTORS\"],-1))\n",
    "                cr_in = (\n",
    "                    world_state[None, :],\n",
    "                    last_done[np.newaxis, :],\n",
    "                )\n",
    "                cr_hstate, value = critic_network.apply(train_states[1].params, hstates[1], cr_in)\n",
    "\n",
    "                # STEP ENV\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                rng_step = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "                obsv, env_state, reward, done, info = jax.vmap(\n",
    "                    env.step, in_axes=(0, 0, 0)\n",
    "                )(rng_step, env_state, env_act)\n",
    "                info = jax.tree_map(lambda x: x.reshape((config[\"NUM_ACTORS\"])), info)\n",
    "                done_batch = batchify(done, env.agents, config[\"NUM_ACTORS\"]).squeeze()\n",
    "                transition = Transition(\n",
    "                    jnp.tile(done[\"__all__\"], env.num_agents),\n",
    "                    done_batch,\n",
    "                    action.squeeze(),\n",
    "                    value.squeeze(),\n",
    "                    batchify(reward, env.agents, config[\"NUM_ACTORS\"]).squeeze(),\n",
    "                    log_prob.squeeze(),\n",
    "                    obs_batch,\n",
    "                    world_state,\n",
    "                    info,\n",
    "                    avail_actions,\n",
    "                )\n",
    "                runner_state = (train_states, env_state, obsv, done_batch, (ac_hstate, cr_hstate), rng)\n",
    "                return runner_state, transition\n",
    "\n",
    "            initial_hstates = runner_state[-2]\n",
    "            runner_state, traj_batch = jax.lax.scan(\n",
    "                _env_step, runner_state, None, config[\"NUM_STEPS\"]\n",
    "            )\n",
    "            \n",
    "            # CALCULATE ADVANTAGE\n",
    "            train_states, env_state, last_obs, last_done, hstates, rng = runner_state\n",
    "      \n",
    "            last_world_state = last_obs[\"world_state\"].reshape((config[\"NUM_ACTORS\"],-1))\n",
    "            cr_in = (\n",
    "                last_world_state[None, :],\n",
    "                last_done[np.newaxis, :],\n",
    "            )\n",
    "            _, last_val = critic_network.apply(train_states[1].params, hstates[1], cr_in)\n",
    "            last_val = last_val.squeeze()\n",
    "\n",
    "            def _calculate_gae(traj_batch, last_val):\n",
    "                def _get_advantages(gae_and_next_value, transition):\n",
    "                    gae, next_value = gae_and_next_value\n",
    "                    done, value, reward = (\n",
    "                        transition.global_done,\n",
    "                        transition.value,\n",
    "                        transition.reward,\n",
    "                    )\n",
    "                    delta = reward + config[\"GAMMA\"] * next_value * (1 - done) - value\n",
    "                    gae = (\n",
    "                        delta\n",
    "                        + config[\"GAMMA\"] * config[\"GAE_LAMBDA\"] * (1 - done) * gae\n",
    "                    )\n",
    "                    return (gae, value), gae\n",
    "\n",
    "                _, advantages = jax.lax.scan(\n",
    "                    _get_advantages,\n",
    "                    (jnp.zeros_like(last_val), last_val),\n",
    "                    traj_batch,\n",
    "                    reverse=True,\n",
    "                    unroll=16,\n",
    "                )\n",
    "                return advantages, advantages + traj_batch.value\n",
    "\n",
    "            advantages, targets = _calculate_gae(traj_batch, last_val)\n",
    "\n",
    "            # UPDATE NETWORK\n",
    "            def _update_epoch(update_state, unused):\n",
    "                def _update_minbatch(train_states, batch_info):\n",
    "                    actor_train_state, critic_train_state = train_states\n",
    "                    ac_init_hstate, cr_init_hstate, traj_batch, advantages, targets = batch_info\n",
    "\n",
    "                    def _actor_loss_fn(actor_params, init_hstate, traj_batch, gae):\n",
    "                        # RERUN NETWORK\n",
    "                        _, pi = actor_network.apply(\n",
    "                            actor_params,\n",
    "                            init_hstate.transpose(),\n",
    "                            (traj_batch.obs, traj_batch.done, traj_batch.avail_actions),\n",
    "                        )\n",
    "                        log_prob = pi.log_prob(traj_batch.action)\n",
    "\n",
    "                        # CALCULATE ACTOR LOSS\n",
    "                        ratio = jnp.exp(log_prob - traj_batch.log_prob)\n",
    "                        gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n",
    "                        loss_actor1 = ratio * gae\n",
    "                        loss_actor2 = (\n",
    "                            jnp.clip(\n",
    "                                ratio,\n",
    "                                1.0 - config[\"CLIP_EPS\"],\n",
    "                                1.0 + config[\"CLIP_EPS\"],\n",
    "                            )\n",
    "                            * gae\n",
    "                        )\n",
    "                        loss_actor = -jnp.minimum(loss_actor1, loss_actor2)\n",
    "                        loss_actor = loss_actor.mean(where=(1 - traj_batch.done))\n",
    "                        entropy = pi.entropy().mean(where=(1 - traj_batch.done))\n",
    "                        actor_loss = (\n",
    "                            loss_actor\n",
    "                            - config[\"ENT_COEF\"] * entropy\n",
    "                        )\n",
    "                        return actor_loss, (loss_actor, entropy)\n",
    "                    \n",
    "                    def _critic_loss_fn(critic_params, init_hstate, traj_batch, targets):\n",
    "                        # RERUN NETWORK\n",
    "                        _, value = critic_network.apply(critic_params, init_hstate.transpose(), (traj_batch.world_state,  traj_batch.done)) \n",
    "                        \n",
    "                        # CALCULATE VALUE LOSS\n",
    "                        value_pred_clipped = traj_batch.value + (\n",
    "                            value - traj_batch.value\n",
    "                        ).clip(-config[\"CLIP_EPS\"], config[\"CLIP_EPS\"])\n",
    "                        value_losses = jnp.square(value - targets)\n",
    "                        value_losses_clipped = jnp.square(value_pred_clipped - targets)\n",
    "                        value_loss = (\n",
    "                            0.5 * jnp.maximum(value_losses, value_losses_clipped).mean(where=(1 - traj_batch.done))\n",
    "                        )\n",
    "                        critic_loss = config[\"VF_COEF\"] * value_loss\n",
    "                        return critic_loss, (value_loss)\n",
    "\n",
    "                    actor_grad_fn = jax.value_and_grad(_actor_loss_fn, has_aux=True)\n",
    "                    actor_loss, actor_grads = actor_grad_fn(\n",
    "                        actor_train_state.params, ac_init_hstate, traj_batch, advantages\n",
    "                    )\n",
    "                    critic_grad_fn = jax.value_and_grad(_critic_loss_fn, has_aux=True)\n",
    "                    critic_loss, critic_grads = critic_grad_fn(\n",
    "                        critic_train_state.params, cr_init_hstate, traj_batch, targets\n",
    "                    )\n",
    "                    \n",
    "                    actor_train_state = actor_train_state.apply_gradients(grads=actor_grads)\n",
    "                    critic_train_state = critic_train_state.apply_gradients(grads=critic_grads)\n",
    "                    \n",
    "                    total_loss = actor_loss[0] + critic_loss[0]\n",
    "                    loss_info = {\n",
    "                        \"total_loss\": total_loss,\n",
    "                        \"actor_loss\": actor_loss[0],\n",
    "                        \"critic_loss\": critic_loss[0],\n",
    "                        \"entropy\": actor_loss[1][1],\n",
    "                    }\n",
    "                    \n",
    "                    return (actor_train_state, critic_train_state), loss_info\n",
    "\n",
    "                (\n",
    "                    train_states,\n",
    "                    init_hstates,\n",
    "                    traj_batch,\n",
    "                    advantages,\n",
    "                    targets,\n",
    "                    rng,\n",
    "                ) = update_state\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "\n",
    "                init_hstates = jax.tree_map(lambda x: jnp.reshape(\n",
    "                    x, (config[\"NUM_STEPS\"], config[\"NUM_ACTORS\"])\n",
    "                ), init_hstates)\n",
    "                \n",
    "                batch = (\n",
    "                    init_hstates[0],\n",
    "                    init_hstates[1],\n",
    "                    traj_batch,\n",
    "                    advantages.squeeze(),\n",
    "                    targets.squeeze(),\n",
    "                )\n",
    "                permutation = jax.random.permutation(_rng, config[\"NUM_ACTORS\"])\n",
    "\n",
    "                shuffled_batch = jax.tree_util.tree_map(\n",
    "                    lambda x: jnp.take(x, permutation, axis=1), batch\n",
    "                )\n",
    "\n",
    "                minibatches = jax.tree_util.tree_map(\n",
    "                    lambda x: jnp.swapaxes(\n",
    "                        jnp.reshape(\n",
    "                            x,\n",
    "                            [x.shape[0], config[\"NUM_MINIBATCHES\"], -1]\n",
    "                            + list(x.shape[2:]),\n",
    "                        ),\n",
    "                        1,\n",
    "                        0,\n",
    "                    ),\n",
    "                    shuffled_batch,\n",
    "                )\n",
    "\n",
    "                train_states, loss_info = jax.lax.scan(\n",
    "                    _update_minbatch, train_states, minibatches\n",
    "                )\n",
    "                update_state = (\n",
    "                    train_states,\n",
    "                    init_hstates,\n",
    "                    traj_batch,\n",
    "                    advantages,\n",
    "                    targets,\n",
    "                    rng,\n",
    "                )\n",
    "                return update_state, loss_info\n",
    "\n",
    "            ac_init_hstate = initial_hstates[0][None, :].squeeze().transpose()\n",
    "            cr_init_hstate = initial_hstates[1][None, :].squeeze().transpose()\n",
    "\n",
    "            update_state = (\n",
    "                train_states,\n",
    "                (ac_init_hstate, cr_init_hstate),\n",
    "                traj_batch,\n",
    "                advantages,\n",
    "                targets,\n",
    "                rng,\n",
    "            )\n",
    "            update_state, loss_info = jax.lax.scan(\n",
    "                _update_epoch, update_state, None, config[\"UPDATE_EPOCHS\"]\n",
    "            )\n",
    "            loss_info = jax.tree_map(lambda x: x.mean(), loss_info)\n",
    "            \n",
    "            train_states = update_state[0]\n",
    "            metric = traj_batch.info\n",
    "            rng = update_state[-1]\n",
    "\n",
    "            # def callback(metric):\n",
    "                \n",
    "            #     wandb.log(\n",
    "            #         {\n",
    "            #             \"returns\": metric[\"returned_episode_returns\"][-1, :].mean(),\n",
    "            #             \"env_step\": metric[\"update_steps\"]\n",
    "            #             * config[\"NUM_ENVS\"]\n",
    "            #             * config[\"NUM_STEPS\"],\n",
    "            #         }\n",
    "            #     )\n",
    "                \n",
    "            \n",
    "            # metric[\"update_steps\"] = update_steps\n",
    "            # jax.experimental.io_callback(callback, None, metric)\n",
    "            update_steps = update_steps + 1\n",
    "            runner_state = (train_states, env_state, last_obs, last_done, hstates, rng)\n",
    "            return (runner_state, update_steps), metric\n",
    "\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        runner_state = (\n",
    "            (actor_train_state, critic_train_state),\n",
    "            env_state,\n",
    "            obsv,\n",
    "            jnp.zeros((config[\"NUM_ACTORS\"]), dtype=bool),\n",
    "            (ac_init_hstate, cr_init_hstate),\n",
    "            _rng,\n",
    "        )\n",
    "        runner_state, metric = jax.lax.scan(\n",
    "            _update_step, (runner_state, 0), None, config[\"NUM_UPDATES\"]\n",
    "        )\n",
    "        return {\"runner_state\": runner_state}\n",
    "\n",
    "    return train\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training function using direct config.\"\"\"\n",
    "    # Use the train_config directly from config.py\n",
    "    config = train_config\n",
    "    \n",
    "    # Initialize wandb\n",
    "    wandb.init(\n",
    "        entity=config[\"ENTITY\"],\n",
    "        project=config[\"PROJECT\"],\n",
    "        tags=[\"MAPPO\", \"RNN\", config[\"ENV_NAME\"]],\n",
    "        config=config,\n",
    "        mode=config[\"WANDB_MODE\"],\n",
    "    )\n",
    "    \n",
    "    # Set random seed\n",
    "    rng = jax.random.PRNGKey(config[\"SEED\"])\n",
    "    \n",
    "    # Run training\n",
    "    with jax.disable_jit(False):\n",
    "        train_jit = jax.jit(make_train(config))\n",
    "        out = train_jit(rng)\n",
    "\n",
    "    return out\n",
    "\n",
    "# Remove Hydra imports and decorators\n",
    "if __name__ == \"__main__\":\n",
    "    test = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from prettytable import PrettyTable\n",
    "from imojify import imojify\n",
    "from data_classes import GameState\n",
    "from config import env_config\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "\n",
    "class TestGame:\n",
    "    def __init__(self, trained_model_state, config):\n",
    "        \"\"\"Initialize test game environment and AI.\"\"\"\n",
    "        self.env = environment_MARL.RL_Roguelike_JAX_MARL()\n",
    "        self.actor_network = ActorRNN(self.env.num_moves, config=config)\n",
    "        self.actor_params = trained_model_state.params\n",
    "        self.hstate = ScannedRNN.initialize_carry(1, 128)\n",
    "        \n",
    "        # Initialize game\n",
    "        key = jax.random.PRNGKey(0)\n",
    "        self.obs, self.state = self.env.reset(key)\n",
    "        self.key = key\n",
    "        \n",
    "        # Initialize action history dataframe\n",
    "        self.action_history = pd.DataFrame(columns=[\n",
    "            'turn', 'player', 'action_id', 'action_name', \n",
    "            'player_health', 'player_action_points', 'player_movement_points', 'player_location',\n",
    "            'enemy_health', 'enemy_action_points', 'enemy_movement_points', 'enemy_location'\n",
    "        ])\n",
    "        \n",
    "        self.turn_start = True\n",
    "        \n",
    "        # Define turn-switching actions\n",
    "        self.end_turn_action = 10  # end turn\n",
    "        self.ability_actions = list(range(11, 11 + env_config['ABILITY_POOL_SIZE']))  # ability picking actions\n",
    "        self.ability_pick_actions = list(range(11, 11 + env_config['ABILITY_POOL_SIZE']))  # Ability picking slots\n",
    "        \n",
    "        self.action_names = [\n",
    "            \"move left down\", \"move left\", \"move left up\",\n",
    "            \"move down\", \"move up\", \n",
    "            \"move right down\", \"move right\", \"move right up\",\n",
    "            \"melee attack\", \"ranged attack\", \"end turn\"\n",
    "        ] + [f\"ability_{i}\" for i in range(env_config['ABILITIES_PER_HERO'])] + [f\"pick_ability_{i}\" for i in range(env_config['ABILITY_POOL_SIZE'])]\n",
    "\n",
    "        # Display initial state\n",
    "        self.render_game_state()\n",
    "\n",
    "    def _record_action(self, action: int, player: str):\n",
    "        \"\"\"Record an action in the action history.\"\"\"\n",
    "        player_idx = 0# if player == \"Player\" else env_config['HEROES_PER_TEAM']\n",
    "        enemy_idx = 1#env_config['HEROES_PER_TEAM'] if player == \"Player\" else 0 # TODO rework\n",
    "        \n",
    "        new_row = {\n",
    "            'turn': self.state.turn_count,\n",
    "            'player': player,\n",
    "            'action_id': action,\n",
    "            'action_name': self.action_names[action],\n",
    "            'player_health': float(self.state.units.health[player_idx][0]),\n",
    "            'player_action_points': float(self.state.units.action_points[player_idx][1]),\n",
    "            'player_movement_points': float(self.state.units.movement_points[player_idx][1]),\n",
    "            'player_location': tuple(self.state.units.location[player_idx]),\n",
    "            'enemy_health': float(self.state.units.health[enemy_idx][0]),\n",
    "            'enemy_action_points': float(self.state.units.action_points[enemy_idx][1]),\n",
    "            'enemy_movement_points': float(self.state.units.movement_points[enemy_idx][1]),\n",
    "            'enemy_location': tuple(self.state.units.location[enemy_idx])\n",
    "        }\n",
    "        \n",
    "        self.action_history = pd.concat([self.action_history, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    def _get_turn_actions(self) -> pd.DataFrame:\n",
    "        \"\"\"Get actions for current/previous turn based on turn_start flag.\"\"\"\n",
    "\n",
    "        return self.action_history[self.action_history['turn'] == self.state.turn_count]\n",
    "\n",
    "\n",
    "    def player_action(self, action: int) -> GameState:\n",
    "        \"\"\"Apply player action and handle AI response if turn ends.\"\"\"\n",
    "        # Validate action\n",
    "        legal_moves = self.env.get_legal_moves(self.state)[\"agent_0\"]\n",
    "        if not legal_moves[action]:\n",
    "            print(f\"Invalid action {action}!\")\n",
    "            # return self.state\n",
    "            \n",
    "        # Apply player action\n",
    "        self.key, subkey = jax.random.split(self.key)\n",
    "        actions = {\"agent_0\": [jnp.int32(action)], \"agent_1\": [jnp.int32(0)]}\n",
    "        self.obs, self.state, reward, done, info = self.env.step(subkey, self.state, actions)\n",
    "        \n",
    "        # Record action AFTER state update\n",
    "        self._record_action(action, \"Player\")\n",
    "        self.turn_start = False\n",
    "            \n",
    "        print(f\"\\nPlayer action: {self.action_names[action]}\")\n",
    "        self.render_game_state()\n",
    "        \n",
    "        # Handle AI turn if appropriate\n",
    "        if self._should_switch_turn(action):\n",
    "            ai_actions = []\n",
    "            while True:\n",
    "                # Get AI action\n",
    "                obs_batch = self.obs[\"agent_1\"]\n",
    "                avail_actions = self.env.get_legal_moves(self.state)[\"agent_1\"]\n",
    "                done_batch = jnp.array(False)\n",
    "                \n",
    "                ai_action, self.hstate = self.get_ai_action(\n",
    "                    obs_batch,\n",
    "                    done_batch,\n",
    "                    avail_actions,\n",
    "                    self.hstate\n",
    "                )\n",
    "                \n",
    "                # Apply AI action\n",
    "                self.key, subkey = jax.random.split(self.key)\n",
    "                actions = {\"agent_0\": [jnp.int32(0)], \"agent_1\": [jnp.int32(ai_action)]}\n",
    "                self.obs, self.state, reward, done, info = self.env.step(\n",
    "                    subkey, self.state, actions\n",
    "                )\n",
    "                \n",
    "                # Record action AFTER state update\n",
    "                self._record_action(ai_action, \"AI\")\n",
    "                ai_actions.append(ai_action)\n",
    "                \n",
    "                if self._should_switch_turn(ai_action):\n",
    "                    break\n",
    "                    \n",
    "            # Print all AI actions at once at end of AI turn\n",
    "            print(\"\\nAI turn:\")\n",
    "            for action in ai_actions:\n",
    "                print(f\"- {self.action_names[action]}\")\n",
    "                \n",
    "            self.turn_start = True\n",
    "            self.render_game_state()\n",
    "                    \n",
    "        # return self.state\n",
    "    \n",
    "    def render_game_state(self):\n",
    "        \"\"\"Renders the current game state.\"\"\"\n",
    "        # Board visualization\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        ax.set_xlim(0, 20)\n",
    "        ax.set_ylim(0, 20)\n",
    "        \n",
    "        # Grid setup\n",
    "        tick_positions = [i + 0.5 for i in range(20)]\n",
    "        ax.set_xticks(tick_positions)\n",
    "        ax.set_yticks(tick_positions)\n",
    "        ax.set_xticklabels([str(i) for i in range(20)])\n",
    "        ax.set_yticklabels([str(i) for i in range(20)])\n",
    "        ax.tick_params(axis='both', which='both', length=0)\n",
    "        ax.set_xticks(range(1, 20), minor=True)\n",
    "        ax.set_yticks(range(1, 20), minor=True)\n",
    "        ax.grid(True, which='minor', linestyle='-', linewidth=1)\n",
    "\n",
    "        # Plot units\n",
    "        for i, location in enumerate(self.state.units.location):\n",
    "            coords = (location[0] + 0.5, location[1] + 0.5)\n",
    "            emoji = 'ðŸ¤—' if i < env_config['HEROES_PER_TEAM'] else 'ðŸ˜ˆ'\n",
    "            img = plt.imread(imojify.get_img_path(emoji))\n",
    "            im = OffsetImage(img, zoom=0.04)\n",
    "            im.image.axes = ax\n",
    "            ab = AnnotationBbox(im, coords, frameon=False, pad=0)\n",
    "            ax.add_artist(ab)\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "        # Game state table\n",
    "        table = PrettyTable()\n",
    "        table.field_names = [\"Attribute\", \"Player\", \"Enemy\"]\n",
    "        \n",
    "        player_idx = 0\n",
    "        enemy_idx = env_config['HEROES_PER_TEAM']\n",
    "        \n",
    "        # Core stats\n",
    "        stats = [\n",
    "            (\"Health\", self.state.units.health[player_idx][0], self.state.units.health[enemy_idx][0]),\n",
    "            (\"Action Points\", self.state.units.action_points[player_idx][1], self.state.units.action_points[enemy_idx][1]),\n",
    "            (\"Movement Points\", self.state.units.movement_points[player_idx][1], self.state.units.movement_points[enemy_idx][1]),\n",
    "            (\"Location\", f\"({self.state.units.location[player_idx][0]}, {self.state.units.location[player_idx][1]})\", \n",
    "                      f\"({self.state.units.location[enemy_idx][0]}, {self.state.units.location[enemy_idx][1]})\"),\n",
    "            (\"Distance\", self.state.distance_to_enemy, \"\"),\n",
    "            (\"Turn\", self.state.turn_count, \"\"),\n",
    "            (\"Steps\", self.state.steps, \"\"),\n",
    "            (\"Pick Mode\", \"Yes\" if self.state.pick_mode else \"No\", \"\")\n",
    "        ]\n",
    "        \n",
    "        # Add abilities\n",
    "        for slot in range(env_config['ABILITIES_PER_HERO']):\n",
    "            p_ability = self.state.units.abilities[player_idx][slot]\n",
    "            e_ability = self.state.units.abilities[enemy_idx][slot]\n",
    "            stats.append((\n",
    "                f\"Ability Slot {slot}\",\n",
    "                f\"ID: {int(p_ability[0])} (CD: {p_ability[2]}/{p_ability[1]})\" if p_ability[0] >= 0 else \"Empty\",\n",
    "                f\"ID: {int(e_ability[0])} (CD: {e_ability[2]}/{e_ability[1]})\" if e_ability[0] >= 0 else \"Empty\"\n",
    "            ))\n",
    "\n",
    "        for name, p_val, e_val in stats:\n",
    "            table.add_row([name, p_val, e_val])\n",
    "\n",
    "        print(\"\\nGame State:\")\n",
    "        print(table)\n",
    "\n",
    "        # Available actions table\n",
    "        action_table = PrettyTable()\n",
    "        action_table.field_names = [\"Action ID\", \"Name\", \"Available\"]\n",
    "\n",
    "        if self.state.pick_mode:\n",
    "            # Show ability picking actions\n",
    "            for i in range(env_config['ABILITY_POOL_SIZE']):\n",
    "                ability = self.state.ability_pool[i]\n",
    "                is_available = not self.state.ability_pool_picked[i]\n",
    "                action_table.add_row([\n",
    "                    i + 11,\n",
    "                    f\"Pick Ability ID: {int(ability[0])} - CD:{int(ability[1])}\",\n",
    "                    \"âœ“\" if is_available else \"âœ—\"\n",
    "                ])\n",
    "        else:\n",
    "            # Show all actions\n",
    "            avail_actions = self.env.get_legal_moves(self.state)[\"agent_0\"]\n",
    "            for i, name in enumerate(self.action_names):\n",
    "                action_table.add_row([\n",
    "                    i,\n",
    "                    name,\n",
    "                    \"âœ“\" if avail_actions[i] else \"âœ—\"\n",
    "                ])\n",
    "\n",
    "        print(\"\\nAvailable Actions:\")\n",
    "        print(action_table)\n",
    "\n",
    "        if self.state.pick_mode:\n",
    "            print(\"\\nAbility Pool:\")\n",
    "            pool_table = PrettyTable()\n",
    "            pool_table.field_names = [\"Index\", \"Ability ID\", \"Cooldown\", \"Parameter 1\", \"Parameter 2\", \"Parameter 3\", \"Status\"]\n",
    "            \n",
    "            for i, ability in enumerate(self.state.ability_pool):\n",
    "                status = \"Picked\" if self.state.ability_pool_picked[i] else \"Available\"\n",
    "                if ability[0] >= 0:\n",
    "                    pool_table.add_row([\n",
    "                        i,\n",
    "                        int(ability[0]),\n",
    "                        int(ability[1]),\n",
    "                        float(ability[3]),\n",
    "                        float(ability[4]),\n",
    "                        float(ability[5]),\n",
    "                        status\n",
    "                    ])\n",
    "            \n",
    "            print(pool_table)\n",
    "\n",
    "        # Action history table\n",
    "        turn_actions = self._get_turn_actions()\n",
    "        if not turn_actions.empty:\n",
    "            print(\"\\nActions this turn:\" if not self.turn_start else \"\\nActions last turn:\")\n",
    "            action_table = PrettyTable()\n",
    "            action_table.field_names = [\"Player\", \"Action\", \"Result\"]\n",
    "            \n",
    "            for _, row in turn_actions.iterrows():\n",
    "                action_table.add_row([\n",
    "                    row['player'],\n",
    "                    f\"{row['action_name']} (ID: {row['action_id']})\",\n",
    "                    f\"HP: {row['player_health']:.1f}, AP: {row['player_action_points']:.1f}, MP: {row['player_movement_points']:.1f}, Loc: {row['player_location']}\"\n",
    "                ])\n",
    "            \n",
    "            print(action_table)\n",
    "    \n",
    "    def _should_switch_turn(self, action: int) -> bool:\n",
    "        \"\"\"Determine if action should trigger turn switch.\n",
    "        \n",
    "        Args:\n",
    "            action: Action index\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if action should switch turns\n",
    "        \"\"\"\n",
    "        return action == self.end_turn_action or action in self.ability_pick_actions\n",
    "    \n",
    "    def get_ai_action(self, obs, done, avail_actions, hstate):\n",
    "        \"\"\"Get action from trained model.\n",
    "        \n",
    "        Args:\n",
    "            obs: Observation array\n",
    "            done: Done flag \n",
    "            avail_actions: Available actions mask\n",
    "            hstate: RNN hidden state\n",
    "            \n",
    "        Returns:\n",
    "            Selected action and new hidden state\n",
    "        \"\"\"\n",
    "        ac_in = (\n",
    "            obs[jnp.newaxis, jnp.newaxis, :],\n",
    "            done[jnp.newaxis, jnp.newaxis],\n",
    "            avail_actions[jnp.newaxis, :]\n",
    "        )\n",
    "        new_hstate, pi = self.actor_network.apply(\n",
    "            self.actor_params, \n",
    "            hstate,\n",
    "            ac_in\n",
    "        )\n",
    "        # Use deterministic action selection for evaluation\n",
    "        action = pi.mode()\n",
    "        return int(action[0,0]), new_hstate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Game State:\n",
      "+-----------------+---------------------+---------------------+\n",
      "|    Attribute    |        Player       |        Enemy        |\n",
      "+-----------------+---------------------+---------------------+\n",
      "|      Health     |        100.0        |        100.0        |\n",
      "|  Action Points  |         3.0         |         3.0         |\n",
      "| Movement Points |         10.0        |         10.0        |\n",
      "|     Location    |        (1, 3)       |       (17, 8)       |\n",
      "|     Distance    |      16.763054      |                     |\n",
      "|       Turn      |          0          |                     |\n",
      "|      Steps      |          0          |                     |\n",
      "|    Pick Mode    |         Yes         |                     |\n",
      "|  Ability Slot 0 | ID: 1 (CD: 0.0/1.0) | ID: 2 (CD: 0.0/1.0) |\n",
      "|  Ability Slot 1 |        Empty        |        Empty        |\n",
      "+-----------------+---------------------+---------------------+\n",
      "\n",
      "Available Actions:\n",
      "+-----------+----------------------------+-----------+\n",
      "| Action ID |            Name            | Available |\n",
      "+-----------+----------------------------+-----------+\n",
      "|     11    | Pick Ability ID: 3 - CD:10 |     âœ“     |\n",
      "|     12    | Pick Ability ID: 5 - CD:20 |     âœ“     |\n",
      "|     13    | Pick Ability ID: 4 - CD:10 |     âœ“     |\n",
      "+-----------+----------------------------+-----------+\n",
      "\n",
      "Ability Pool:\n",
      "+-------+------------+----------+-------------+-------------+-------------+-----------+\n",
      "| Index | Ability ID | Cooldown | Parameter 1 | Parameter 2 | Parameter 3 |   Status  |\n",
      "+-------+------------+----------+-------------+-------------+-------------+-----------+\n",
      "|   0   |     3      |    10    |     0.0     |     0.0     |     0.0     | Available |\n",
      "|   1   |     5      |    20    |     5.0     |     0.0     |     0.0     | Available |\n",
      "|   2   |     4      |    10    |     5.0     |     0.0     |     0.0     | Available |\n",
      "+-------+------------+----------+-------------+-------------+-------------+-----------+\n"
     ]
    }
   ],
   "source": [
    "test_game = TestGame(test[\"runner_state\"][0][0][0], train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_game.player_action(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_game.player_action(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_game.player_action(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_game.player_action(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_game.player_action(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_game.player_action(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_game.player_action(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_game.player_action(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_game.player_action(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_game.env.num_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_game.state.units.available_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_game.player_action(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_game.player_action(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_game.player_action(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_game.player_action(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_game.player_action(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_game.player_action(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_game.player_action(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_game.player_action(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_game.player_action(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL-Roguelike-JAX-MARL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
