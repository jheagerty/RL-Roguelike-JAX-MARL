{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    }
   ],
   "source": [
    "#development.ipynb\n",
    "import environment_MARL\n",
    "import data_classes\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "from flax.linen.initializers import constant, orthogonal\n",
    "from typing import Sequence, NamedTuple, Any, Tuple, Union, Dict\n",
    "import distrax\n",
    "\n",
    "\n",
    "import functools\n",
    "\n",
    "class ScannedRNN(nn.Module):\n",
    "    @functools.partial(\n",
    "        nn.scan,\n",
    "        variable_broadcast=\"params\",\n",
    "        in_axes=0,\n",
    "        out_axes=0,\n",
    "        split_rngs={\"params\": False},\n",
    "    )\n",
    "    @nn.compact\n",
    "    def __call__(self, carry, x):\n",
    "        \"\"\"Applies the module.\"\"\"\n",
    "        rnn_state = carry\n",
    "        ins, resets = x\n",
    "        rnn_state = jnp.where(\n",
    "            resets[:, np.newaxis],\n",
    "            self.initialize_carry(ins.shape[0], ins.shape[1]),\n",
    "            rnn_state,\n",
    "        )\n",
    "        new_rnn_state, y = nn.GRUCell(features=ins.shape[1])(rnn_state, ins)\n",
    "        return new_rnn_state, y\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_carry(batch_size, hidden_size):\n",
    "        # Use a dummy key since the default state init fn is just zeros.\n",
    "        cell = nn.GRUCell(features=hidden_size)\n",
    "        return cell.initialize_carry(jax.random.PRNGKey(0), (batch_size, hidden_size))\n",
    "\n",
    "\n",
    "class ActorRNN(nn.Module):\n",
    "    action_dim: Sequence[int]\n",
    "    config: Dict\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, hidden, x):\n",
    "        obs, dones, avail_actions = x\n",
    "        embedding = nn.Dense(\n",
    "            128, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(obs)\n",
    "        embedding = nn.relu(embedding)\n",
    "\n",
    "        rnn_in = (embedding, dones)\n",
    "        hidden, embedding = ScannedRNN()(hidden, rnn_in)\n",
    "\n",
    "        actor_mean = nn.Dense(128, kernel_init=orthogonal(2), bias_init=constant(0.0))(\n",
    "            embedding\n",
    "        )\n",
    "        actor_mean = nn.relu(actor_mean)\n",
    "        action_logits = nn.Dense(\n",
    "            self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        unavail_actions = 1 - avail_actions\n",
    "        action_logits = action_logits - (unavail_actions * 1e10)\n",
    "\n",
    "        pi = distrax.Categorical(logits=action_logits)\n",
    "\n",
    "        return hidden, pi\n",
    "\n",
    "\n",
    "class CriticRNN(nn.Module):\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, hidden, x):\n",
    "        world_state, dones = x\n",
    "        embedding = nn.Dense(\n",
    "            128, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(world_state)\n",
    "        embedding = nn.relu(embedding)\n",
    "        \n",
    "        rnn_in = (embedding, dones)\n",
    "        hidden, embedding = ScannedRNN()(hidden, rnn_in)\n",
    "        \n",
    "        critic = nn.Dense(128, kernel_init=orthogonal(2), bias_init=constant(0.0))(\n",
    "            embedding\n",
    "        )\n",
    "        critic = nn.relu(critic)\n",
    "        critic = nn.Dense(1, kernel_init=orthogonal(1.0), bias_init=constant(0.0))(\n",
    "            critic\n",
    "        )\n",
    "        \n",
    "        return hidden, jnp.squeeze(critic, axis=-1)\n",
    "    \n",
    "from jaxmarl.wrappers.baselines import JaxMARLWrapper\n",
    "from functools import partial\n",
    "\n",
    "class HanabiWorldStateWrapper(JaxMARLWrapper):\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def reset(self,\n",
    "              key):\n",
    "        obs, env_state = self._env.reset(key)\n",
    "        obs[\"world_state\"] = self.world_state(obs, env_state)\n",
    "        return obs, env_state\n",
    "    \n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def step(self,\n",
    "             key,\n",
    "             state,\n",
    "             action):\n",
    "        obs, env_state, reward, done, info = self._env.step(\n",
    "            key, state, action\n",
    "        )\n",
    "        obs[\"world_state\"] = self.world_state(obs, state)\n",
    "        return obs, env_state, reward, done, info\n",
    "\n",
    "    @partial(jax.jit, static_argnums=0)\n",
    "    def world_state(self, obs, state):\n",
    "        \"\"\" \n",
    "        For each agent: [agent obs, own hand]\n",
    "        \"\"\"\n",
    "            \n",
    "        all_obs = jnp.array([obs[agent] for agent in self._env.agents])\n",
    "        # hands = state.player_hands.reshape((self._env.num_agents, -1))\n",
    "        return all_obs\n",
    "        \n",
    "    \n",
    "    def world_state_size(self):\n",
    "   \n",
    "        return data_classes.get_observation_size(data_classes.schema) # NOTE hardcoded hand size\n",
    "    \n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "from typing import Sequence, NamedTuple, Any, Tuple, Union, Dict\n",
    "\n",
    "from flax.training.train_state import TrainState\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import jaxmarl\n",
    "from jaxmarl.wrappers.baselines import LogWrapper\n",
    "\n",
    "import wandb\n",
    "from config import train_config\n",
    "\n",
    "class Transition(NamedTuple):\n",
    "    global_done: jnp.ndarray\n",
    "    done: jnp.ndarray\n",
    "    action: jnp.ndarray\n",
    "    value: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    log_prob: jnp.ndarray\n",
    "    obs: jnp.ndarray\n",
    "    world_state: jnp.ndarray\n",
    "    info: jnp.ndarray\n",
    "    avail_actions: jnp.ndarray\n",
    "\n",
    "\n",
    "def batchify(x: dict, agent_list, num_actors):\n",
    "    x = jnp.stack([x[a] for a in agent_list])\n",
    "    return x.reshape((num_actors, -1))\n",
    "\n",
    "\n",
    "def unbatchify(x: jnp.ndarray, agent_list, num_envs, num_actors):\n",
    "    x = x.reshape((num_actors, num_envs, -1))\n",
    "    return {a: x[i] for i, a in enumerate(agent_list)}\n",
    "\n",
    "\n",
    "def make_train(config):\n",
    "    env = environment_MARL.RL_Roguelike_JAX_MARL()\n",
    "    num_actions = env.num_moves  # Get actual number of actions from environment\n",
    "    config[\"NUM_ACTORS\"] = env.num_agents * config[\"NUM_ENVS\"]\n",
    "    config[\"NUM_UPDATES\"] = (\n",
    "        config[\"TOTAL_TIMESTEPS\"] // config[\"NUM_STEPS\"] // config[\"NUM_ENVS\"]\n",
    "    )\n",
    "    config[\"MINIBATCH_SIZE\"] = (\n",
    "        config[\"NUM_ACTORS\"] * config[\"NUM_STEPS\"] // config[\"NUM_MINIBATCHES\"]\n",
    "    )\n",
    "    config[\"CLIP_EPS\"] = config[\"CLIP_EPS\"] / env.num_agents if config[\"SCALE_CLIP_EPS\"] else config[\"CLIP_EPS\"]\n",
    "\n",
    "    # env = FlattenObservationWrapper(env) # NOTE need a batchify wrapper\n",
    "    env = HanabiWorldStateWrapper(env)\n",
    "    env = LogWrapper(env)\n",
    "\n",
    "    def linear_schedule(count):\n",
    "        frac = (\n",
    "            1.0\n",
    "            - (count // (config[\"NUM_MINIBATCHES\"] * config[\"UPDATE_EPOCHS\"]))\n",
    "            / config[\"NUM_UPDATES\"]\n",
    "        )\n",
    "        return config[\"LR\"] * frac\n",
    "\n",
    "    def train(rng):\n",
    "        # INIT NETWORK\n",
    "        actor_network = ActorRNN(num_actions, config=config)\n",
    "        critic_network = CriticRNN()\n",
    "        rng, _rng_actor, _rng_critic = jax.random.split(rng, 3)\n",
    "        ac_init_x = (\n",
    "            jnp.zeros((1, config[\"NUM_ENVS\"], data_classes.get_observation_size(data_classes.schema))),\n",
    "            jnp.zeros((1, config[\"NUM_ENVS\"])),\n",
    "            jnp.zeros((1, config[\"NUM_ENVS\"], num_actions)),\n",
    "        )\n",
    "        ac_init_hstate = ScannedRNN.initialize_carry(config[\"NUM_ENVS\"], 128)\n",
    "        actor_network_params = actor_network.init(_rng_actor, ac_init_hstate, ac_init_x)\n",
    "        \n",
    "        cr_init_x = (\n",
    "            jnp.zeros((1, config[\"NUM_ENVS\"], env.world_state_size(),)), \n",
    "            jnp.zeros((1, config[\"NUM_ENVS\"])),\n",
    "        )\n",
    "        cr_init_hstate = ScannedRNN.initialize_carry(config[\"NUM_ENVS\"], 128)\n",
    "        critic_network_params = critic_network.init(_rng_critic, cr_init_hstate, cr_init_x)\n",
    "        \n",
    "        if config[\"ANNEAL_LR\"]:\n",
    "            actor_tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(learning_rate=linear_schedule, eps=1e-5),\n",
    "            )\n",
    "            critic_tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(learning_rate=linear_schedule, eps=1e-5),\n",
    "            )\n",
    "        else:\n",
    "            actor_tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(config[\"LR\"], eps=1e-5),\n",
    "            )\n",
    "            critic_tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(config[\"LR\"], eps=1e-5),\n",
    "            )\n",
    "        actor_train_state = TrainState.create(\n",
    "            apply_fn=actor_network.apply,\n",
    "            params=actor_network_params,\n",
    "            tx=actor_tx,\n",
    "        )\n",
    "        critic_train_state = TrainState.create(\n",
    "            apply_fn=actor_network.apply,\n",
    "            params=critic_network_params,\n",
    "            tx=critic_tx,\n",
    "        )\n",
    "\n",
    "        # INIT ENV\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        reset_rng = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "        obsv, env_state = jax.vmap(env.reset, in_axes=(0,))(reset_rng)\n",
    "        ac_init_hstate = ScannedRNN.initialize_carry(config[\"NUM_ACTORS\"], 128)\n",
    "        cr_init_hstate = ScannedRNN.initialize_carry(config[\"NUM_ACTORS\"], 128)\n",
    "\n",
    "        # TRAIN LOOP\n",
    "        def _update_step(update_runner_state, unused):\n",
    "            # COLLECT TRAJECTORIES\n",
    "            runner_state, update_steps = update_runner_state\n",
    "            \n",
    "            def _env_step(runner_state, unused):\n",
    "                train_states, env_state, last_obs, last_done, hstates, rng = runner_state\n",
    "\n",
    "                # SELECT ACTION\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                avail_actions = jax.vmap(env.get_legal_moves)(env_state.env_state)\n",
    "                avail_actions = jax.lax.stop_gradient(\n",
    "                    batchify(avail_actions, env.agents, config[\"NUM_ACTORS\"])\n",
    "                )\n",
    "                obs_batch = batchify(last_obs, env.agents, config[\"NUM_ACTORS\"])\n",
    "                ac_in = (\n",
    "                    obs_batch[np.newaxis, :],\n",
    "                    last_done[np.newaxis, :],\n",
    "                    avail_actions,\n",
    "                )\n",
    "                ac_hstate, pi = actor_network.apply(train_states[0].params, hstates[0], ac_in)\n",
    "                action = pi.sample(seed=_rng)\n",
    "                log_prob = pi.log_prob(action)\n",
    "                env_act = unbatchify(\n",
    "                    action, env.agents, config[\"NUM_ENVS\"], env.num_agents\n",
    "                )\n",
    "\n",
    "                # VALUE\n",
    "                world_state = last_obs[\"world_state\"].reshape((config[\"NUM_ACTORS\"],-1))\n",
    "                cr_in = (\n",
    "                    world_state[None, :],\n",
    "                    last_done[np.newaxis, :],\n",
    "                )\n",
    "                cr_hstate, value = critic_network.apply(train_states[1].params, hstates[1], cr_in)\n",
    "\n",
    "                # STEP ENV\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                rng_step = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "                obsv, env_state, reward, done, info = jax.vmap(\n",
    "                    env.step, in_axes=(0, 0, 0)\n",
    "                )(rng_step, env_state, env_act)\n",
    "                info = jax.tree_map(lambda x: x.reshape((config[\"NUM_ACTORS\"])), info)\n",
    "                done_batch = batchify(done, env.agents, config[\"NUM_ACTORS\"]).squeeze()\n",
    "                transition = Transition(\n",
    "                    jnp.tile(done[\"__all__\"], env.num_agents),\n",
    "                    done_batch,\n",
    "                    action.squeeze(),\n",
    "                    value.squeeze(),\n",
    "                    batchify(reward, env.agents, config[\"NUM_ACTORS\"]).squeeze(),\n",
    "                    log_prob.squeeze(),\n",
    "                    obs_batch,\n",
    "                    world_state,\n",
    "                    info,\n",
    "                    avail_actions,\n",
    "                )\n",
    "                runner_state = (train_states, env_state, obsv, done_batch, (ac_hstate, cr_hstate), rng)\n",
    "                return runner_state, transition\n",
    "\n",
    "            initial_hstates = runner_state[-2]\n",
    "            runner_state, traj_batch = jax.lax.scan(\n",
    "                _env_step, runner_state, None, config[\"NUM_STEPS\"]\n",
    "            )\n",
    "            \n",
    "            # CALCULATE ADVANTAGE\n",
    "            train_states, env_state, last_obs, last_done, hstates, rng = runner_state\n",
    "      \n",
    "            last_world_state = last_obs[\"world_state\"].reshape((config[\"NUM_ACTORS\"],-1))\n",
    "            cr_in = (\n",
    "                last_world_state[None, :],\n",
    "                last_done[np.newaxis, :],\n",
    "            )\n",
    "            _, last_val = critic_network.apply(train_states[1].params, hstates[1], cr_in)\n",
    "            last_val = last_val.squeeze()\n",
    "\n",
    "            def _calculate_gae(traj_batch, last_val):\n",
    "                def _get_advantages(gae_and_next_value, transition):\n",
    "                    gae, next_value = gae_and_next_value\n",
    "                    done, value, reward = (\n",
    "                        transition.global_done,\n",
    "                        transition.value,\n",
    "                        transition.reward,\n",
    "                    )\n",
    "                    delta = reward + config[\"GAMMA\"] * next_value * (1 - done) - value\n",
    "                    gae = (\n",
    "                        delta\n",
    "                        + config[\"GAMMA\"] * config[\"GAE_LAMBDA\"] * (1 - done) * gae\n",
    "                    )\n",
    "                    return (gae, value), gae\n",
    "\n",
    "                _, advantages = jax.lax.scan(\n",
    "                    _get_advantages,\n",
    "                    (jnp.zeros_like(last_val), last_val),\n",
    "                    traj_batch,\n",
    "                    reverse=True,\n",
    "                    unroll=16,\n",
    "                )\n",
    "                return advantages, advantages + traj_batch.value\n",
    "\n",
    "            advantages, targets = _calculate_gae(traj_batch, last_val)\n",
    "\n",
    "            # UPDATE NETWORK\n",
    "            def _update_epoch(update_state, unused):\n",
    "                def _update_minbatch(train_states, batch_info):\n",
    "                    actor_train_state, critic_train_state = train_states\n",
    "                    ac_init_hstate, cr_init_hstate, traj_batch, advantages, targets = batch_info\n",
    "\n",
    "                    def _actor_loss_fn(actor_params, init_hstate, traj_batch, gae):\n",
    "                        # RERUN NETWORK\n",
    "                        _, pi = actor_network.apply(\n",
    "                            actor_params,\n",
    "                            init_hstate.transpose(),\n",
    "                            (traj_batch.obs, traj_batch.done, traj_batch.avail_actions),\n",
    "                        )\n",
    "                        log_prob = pi.log_prob(traj_batch.action)\n",
    "\n",
    "                        # CALCULATE ACTOR LOSS\n",
    "                        ratio = jnp.exp(log_prob - traj_batch.log_prob)\n",
    "                        gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n",
    "                        loss_actor1 = ratio * gae\n",
    "                        loss_actor2 = (\n",
    "                            jnp.clip(\n",
    "                                ratio,\n",
    "                                1.0 - config[\"CLIP_EPS\"],\n",
    "                                1.0 + config[\"CLIP_EPS\"],\n",
    "                            )\n",
    "                            * gae\n",
    "                        )\n",
    "                        loss_actor = -jnp.minimum(loss_actor1, loss_actor2)\n",
    "                        loss_actor = loss_actor.mean(where=(1 - traj_batch.done))\n",
    "                        entropy = pi.entropy().mean(where=(1 - traj_batch.done))\n",
    "                        actor_loss = (\n",
    "                            loss_actor\n",
    "                            - config[\"ENT_COEF\"] * entropy\n",
    "                        )\n",
    "                        return actor_loss, (loss_actor, entropy)\n",
    "                    \n",
    "                    def _critic_loss_fn(critic_params, init_hstate, traj_batch, targets):\n",
    "                        # RERUN NETWORK\n",
    "                        _, value = critic_network.apply(critic_params, init_hstate.transpose(), (traj_batch.world_state,  traj_batch.done)) \n",
    "                        \n",
    "                        # CALCULATE VALUE LOSS\n",
    "                        value_pred_clipped = traj_batch.value + (\n",
    "                            value - traj_batch.value\n",
    "                        ).clip(-config[\"CLIP_EPS\"], config[\"CLIP_EPS\"])\n",
    "                        value_losses = jnp.square(value - targets)\n",
    "                        value_losses_clipped = jnp.square(value_pred_clipped - targets)\n",
    "                        value_loss = (\n",
    "                            0.5 * jnp.maximum(value_losses, value_losses_clipped).mean(where=(1 - traj_batch.done))\n",
    "                        )\n",
    "                        critic_loss = config[\"VF_COEF\"] * value_loss\n",
    "                        return critic_loss, (value_loss)\n",
    "\n",
    "                    actor_grad_fn = jax.value_and_grad(_actor_loss_fn, has_aux=True)\n",
    "                    actor_loss, actor_grads = actor_grad_fn(\n",
    "                        actor_train_state.params, ac_init_hstate, traj_batch, advantages\n",
    "                    )\n",
    "                    critic_grad_fn = jax.value_and_grad(_critic_loss_fn, has_aux=True)\n",
    "                    critic_loss, critic_grads = critic_grad_fn(\n",
    "                        critic_train_state.params, cr_init_hstate, traj_batch, targets\n",
    "                    )\n",
    "                    \n",
    "                    actor_train_state = actor_train_state.apply_gradients(grads=actor_grads)\n",
    "                    critic_train_state = critic_train_state.apply_gradients(grads=critic_grads)\n",
    "                    \n",
    "                    total_loss = actor_loss[0] + critic_loss[0]\n",
    "                    loss_info = {\n",
    "                        \"total_loss\": total_loss,\n",
    "                        \"actor_loss\": actor_loss[0],\n",
    "                        \"critic_loss\": critic_loss[0],\n",
    "                        \"entropy\": actor_loss[1][1],\n",
    "                    }\n",
    "                    \n",
    "                    return (actor_train_state, critic_train_state), loss_info\n",
    "\n",
    "                (\n",
    "                    train_states,\n",
    "                    init_hstates,\n",
    "                    traj_batch,\n",
    "                    advantages,\n",
    "                    targets,\n",
    "                    rng,\n",
    "                ) = update_state\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "\n",
    "                init_hstates = jax.tree_map(lambda x: jnp.reshape(\n",
    "                    x, (config[\"NUM_STEPS\"], config[\"NUM_ACTORS\"])\n",
    "                ), init_hstates)\n",
    "                \n",
    "                batch = (\n",
    "                    init_hstates[0],\n",
    "                    init_hstates[1],\n",
    "                    traj_batch,\n",
    "                    advantages.squeeze(),\n",
    "                    targets.squeeze(),\n",
    "                )\n",
    "                permutation = jax.random.permutation(_rng, config[\"NUM_ACTORS\"])\n",
    "\n",
    "                shuffled_batch = jax.tree_util.tree_map(\n",
    "                    lambda x: jnp.take(x, permutation, axis=1), batch\n",
    "                )\n",
    "\n",
    "                minibatches = jax.tree_util.tree_map(\n",
    "                    lambda x: jnp.swapaxes(\n",
    "                        jnp.reshape(\n",
    "                            x,\n",
    "                            [x.shape[0], config[\"NUM_MINIBATCHES\"], -1]\n",
    "                            + list(x.shape[2:]),\n",
    "                        ),\n",
    "                        1,\n",
    "                        0,\n",
    "                    ),\n",
    "                    shuffled_batch,\n",
    "                )\n",
    "\n",
    "                train_states, loss_info = jax.lax.scan(\n",
    "                    _update_minbatch, train_states, minibatches\n",
    "                )\n",
    "                update_state = (\n",
    "                    train_states,\n",
    "                    init_hstates,\n",
    "                    traj_batch,\n",
    "                    advantages,\n",
    "                    targets,\n",
    "                    rng,\n",
    "                )\n",
    "                return update_state, loss_info\n",
    "\n",
    "            ac_init_hstate = initial_hstates[0][None, :].squeeze().transpose()\n",
    "            cr_init_hstate = initial_hstates[1][None, :].squeeze().transpose()\n",
    "\n",
    "            update_state = (\n",
    "                train_states,\n",
    "                (ac_init_hstate, cr_init_hstate),\n",
    "                traj_batch,\n",
    "                advantages,\n",
    "                targets,\n",
    "                rng,\n",
    "            )\n",
    "            update_state, loss_info = jax.lax.scan(\n",
    "                _update_epoch, update_state, None, config[\"UPDATE_EPOCHS\"]\n",
    "            )\n",
    "            loss_info = jax.tree_map(lambda x: x.mean(), loss_info)\n",
    "            \n",
    "            train_states = update_state[0]\n",
    "            metric = traj_batch.info\n",
    "            rng = update_state[-1]\n",
    "\n",
    "            # def callback(metric):\n",
    "                \n",
    "            #     wandb.log(\n",
    "            #         {\n",
    "            #             \"returns\": metric[\"returned_episode_returns\"][-1, :].mean(),\n",
    "            #             \"env_step\": metric[\"update_steps\"]\n",
    "            #             * config[\"NUM_ENVS\"]\n",
    "            #             * config[\"NUM_STEPS\"],\n",
    "            #         }\n",
    "            #     )\n",
    "                \n",
    "            \n",
    "            # metric[\"update_steps\"] = update_steps\n",
    "            # jax.experimental.io_callback(callback, None, metric)\n",
    "            update_steps = update_steps + 1\n",
    "            runner_state = (train_states, env_state, last_obs, last_done, hstates, rng)\n",
    "            return (runner_state, update_steps), metric\n",
    "\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        runner_state = (\n",
    "            (actor_train_state, critic_train_state),\n",
    "            env_state,\n",
    "            obsv,\n",
    "            jnp.zeros((config[\"NUM_ACTORS\"]), dtype=bool),\n",
    "            (ac_init_hstate, cr_init_hstate),\n",
    "            _rng,\n",
    "        )\n",
    "        runner_state, metric = jax.lax.scan(\n",
    "            _update_step, (runner_state, 0), None, config[\"NUM_UPDATES\"]\n",
    "        )\n",
    "        return {\"runner_state\": runner_state}\n",
    "\n",
    "    return train\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training function using direct config.\"\"\"\n",
    "    # Use the train_config directly from config.py\n",
    "    config = train_config\n",
    "    \n",
    "    # Initialize wandb\n",
    "    wandb.init(\n",
    "        entity=config[\"ENTITY\"],\n",
    "        project=config[\"PROJECT\"],\n",
    "        tags=[\"MAPPO\", \"RNN\", config[\"ENV_NAME\"]],\n",
    "        config=config,\n",
    "        mode=config[\"WANDB_MODE\"],\n",
    "    )\n",
    "    \n",
    "    # Set random seed\n",
    "    rng = jax.random.PRNGKey(config[\"SEED\"])\n",
    "    \n",
    "    # Run training\n",
    "    with jax.disable_jit(False):\n",
    "        train_jit = jax.jit(make_train(config))\n",
    "        out = train_jit(rng)\n",
    "\n",
    "    return out\n",
    "\n",
    "# Remove Hydra imports and decorators\n",
    "if __name__ == \"__main__\":\n",
    "    test = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([100., 100., 100., 100., 100., 100., 100., 100., 100., 100., 100.,\n",
       "       100., 100., 100., 100.,  40.,  10., 100.,  85., 100.,  25., 100.,\n",
       "       100., 100., 100., 100., 100.,  40., 100., 100., 100., 100., 100.,\n",
       "       100., 100.,  70.,  25., 100., 100., 100., 100., 100., 100., 100.,\n",
       "        85., 100.,  40.,  25., 100.,  40., 100., 100.,  40., 100., 100.,\n",
       "       100., 100.,  85.,  40., 100., 100.,  85., 100.,  25., 100., 100.,\n",
       "       100., 100., 100., 100., 100.,  25.,  85.,  25., 100.,  70., 100.,\n",
       "       100., 100.,  40., 100.,  55., 100., 100., 100., 100., 100.,  85.,\n",
       "        25.,  85.,  25., 100.,   0., 100., 100., 100., 100., 100., 100.,\n",
       "       100., 100., 100., 100., 100., 100.,  40., 100., 100., 100.,  25.,\n",
       "        85., 100., 100., 100.,  70., 100., 100., 100., 100., 100., 100.,\n",
       "       100., 100.,  85., 100., 100.,  85., 100.], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['runner_state'][0][1].env_state.player.health_current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "       0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['runner_state'][0][1].env_state.player.ability_state_1.ability_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([5., 5., 5., 5., 3., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5.,\n",
       "       5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5.,\n",
       "       5., 5., 3., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5.,\n",
       "       5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5.,\n",
       "       5., 2., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5.,\n",
       "       5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5.,\n",
       "       5., 5., 5., 1., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5.,\n",
       "       5., 5., 5., 3., 5., 4., 5., 5., 5.], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['runner_state'][0][1].env_state.player.action_points_current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([5.        , 0.1715728 , 3.        , 3.5857863 , 2.5857863 ,\n",
       "       4.        , 5.        , 5.        , 2.5857863 , 3.5857863 ,\n",
       "       0.17157292, 0.1715728 , 5.        , 5.        , 5.        ,\n",
       "       4.        , 0.1715728 , 5.        , 5.        , 5.        ,\n",
       "       5.        , 5.        , 0.1715728 , 5.        , 3.        ,\n",
       "       5.        , 1.1715728 , 5.        , 5.        , 5.        ,\n",
       "       2.5857863 , 4.        , 1.1715728 , 5.        , 0.75735915,\n",
       "       5.        , 0.17157269, 5.        , 5.        , 5.        ,\n",
       "       5.        , 5.        , 2.5857863 , 5.        , 5.        ,\n",
       "       5.        , 5.        , 5.        , 5.        , 5.        ,\n",
       "       5.        , 5.        , 5.        , 5.        , 5.        ,\n",
       "       5.        , 5.        , 5.        , 2.5857863 , 2.5857863 ,\n",
       "       4.        , 5.        , 5.        , 5.        , 4.        ,\n",
       "       2.1715727 , 4.        , 0.1715728 , 5.        , 1.1715728 ,\n",
       "       5.        , 5.        , 5.        , 3.        , 2.5857863 ,\n",
       "       5.        , 0.17157292, 5.        , 5.        , 3.        ,\n",
       "       5.        , 3.5857863 , 0.17157269, 5.        , 5.        ,\n",
       "       5.        , 5.        , 5.        , 4.        , 5.        ,\n",
       "       5.        , 0.58578646, 5.        , 4.        , 4.        ,\n",
       "       5.        , 5.        , 5.        , 4.        , 5.        ,\n",
       "       5.        , 5.        , 2.5857863 , 5.        , 3.5857863 ,\n",
       "       3.5857863 , 5.        , 5.        , 5.        , 5.        ,\n",
       "       5.        , 2.        , 5.        , 3.        , 2.5857863 ,\n",
       "       0.        , 5.        , 5.        , 5.        , 5.        ,\n",
       "       5.        , 5.        , 3.        , 3.5857863 , 2.5857863 ,\n",
       "       5.        , 5.        , 5.        ], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['runner_state'][0][1].env_state.player.movement_points_current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 3, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['runner_state'][0][1].env_state.player.ability_state_1.current_cooldown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import environment_MARL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import jax\n",
    "# import jax.numpy as jnp\n",
    "# import flax.linen as nn\n",
    "# import numpy as np\n",
    "# from flax.linen.initializers import constant, orthogonal\n",
    "# from typing import Sequence, NamedTuple, Any, Tuple, Union, Dict\n",
    "# import distrax\n",
    "\n",
    "\n",
    "# import functools\n",
    "\n",
    "# class ScannedRNN(nn.Module):\n",
    "#     @functools.partial(\n",
    "#         nn.scan,\n",
    "#         variable_broadcast=\"params\",\n",
    "#         in_axes=0,\n",
    "#         out_axes=0,\n",
    "#         split_rngs={\"params\": False},\n",
    "#     )\n",
    "#     @nn.compact\n",
    "#     def __call__(self, carry, x):\n",
    "#         \"\"\"Applies the module.\"\"\"\n",
    "#         rnn_state = carry\n",
    "#         ins, resets = x\n",
    "#         rnn_state = jnp.where(\n",
    "#             resets[:, np.newaxis],\n",
    "#             self.initialize_carry(ins.shape[0], ins.shape[1]),\n",
    "#             rnn_state,\n",
    "#         )\n",
    "#         new_rnn_state, y = nn.GRUCell(features=ins.shape[1])(rnn_state, ins)\n",
    "#         return new_rnn_state, y\n",
    "\n",
    "#     @staticmethod\n",
    "#     def initialize_carry(batch_size, hidden_size):\n",
    "#         # Use a dummy key since the default state init fn is just zeros.\n",
    "#         cell = nn.GRUCell(features=hidden_size)\n",
    "#         return cell.initialize_carry(jax.random.PRNGKey(0), (batch_size, hidden_size))\n",
    "\n",
    "\n",
    "# class ActorRNN(nn.Module):\n",
    "#     action_dim: Sequence[int]\n",
    "#     config: Dict\n",
    "\n",
    "#     @nn.compact\n",
    "#     def __call__(self, hidden, x):\n",
    "#         obs, dones, avail_actions = x\n",
    "#         embedding = nn.Dense(\n",
    "#             128, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "#         )(obs)\n",
    "#         embedding = nn.relu(embedding)\n",
    "\n",
    "#         rnn_in = (embedding, dones)\n",
    "#         hidden, embedding = ScannedRNN()(hidden, rnn_in)\n",
    "\n",
    "#         actor_mean = nn.Dense(128, kernel_init=orthogonal(2), bias_init=constant(0.0))(\n",
    "#             embedding\n",
    "#         )\n",
    "#         actor_mean = nn.relu(actor_mean)\n",
    "#         action_logits = nn.Dense(\n",
    "#             self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0)\n",
    "#         )(actor_mean)\n",
    "#         unavail_actions = 1 - avail_actions\n",
    "#         action_logits = action_logits - (unavail_actions * 1e10)\n",
    "\n",
    "#         pi = distrax.Categorical(logits=action_logits)\n",
    "\n",
    "#         return hidden, pi\n",
    "\n",
    "\n",
    "# class CriticRNN(nn.Module):\n",
    "    \n",
    "#     @nn.compact\n",
    "#     def __call__(self, hidden, x):\n",
    "#         world_state, dones = x\n",
    "#         embedding = nn.Dense(\n",
    "#             128, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "#         )(world_state)\n",
    "#         embedding = nn.relu(embedding)\n",
    "        \n",
    "#         rnn_in = (embedding, dones)\n",
    "#         hidden, embedding = ScannedRNN()(hidden, rnn_in)\n",
    "        \n",
    "#         critic = nn.Dense(128, kernel_init=orthogonal(2), bias_init=constant(0.0))(\n",
    "#             embedding\n",
    "#         )\n",
    "#         critic = nn.relu(critic)\n",
    "#         critic = nn.Dense(1, kernel_init=orthogonal(1.0), bias_init=constant(0.0))(\n",
    "#             critic\n",
    "#         )\n",
    "        \n",
    "#         return hidden, jnp.squeeze(critic, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from jaxmarl.wrappers.baselines import JaxMARLWrapper\n",
    "# from functools import partial\n",
    "\n",
    "# class HanabiWorldStateWrapper(JaxMARLWrapper):\n",
    "    \n",
    "#     @partial(jax.jit, static_argnums=0)\n",
    "#     def reset(self,\n",
    "#               key):\n",
    "#         obs, env_state = self._env.reset(key)\n",
    "#         obs[\"world_state\"] = self.world_state(obs, env_state)\n",
    "#         return obs, env_state\n",
    "    \n",
    "#     @partial(jax.jit, static_argnums=0)\n",
    "#     def step(self,\n",
    "#              key,\n",
    "#              state,\n",
    "#              action):\n",
    "#         obs, env_state, reward, done, info = self._env.step(\n",
    "#             key, state, action\n",
    "#         )\n",
    "#         obs[\"world_state\"] = self.world_state(obs, state)\n",
    "#         return obs, env_state, reward, done, info\n",
    "\n",
    "#     @partial(jax.jit, static_argnums=0)\n",
    "#     def world_state(self, obs, state):\n",
    "#         \"\"\" \n",
    "#         For each agent: [agent obs, own hand]\n",
    "#         \"\"\"\n",
    "            \n",
    "#         all_obs = jnp.array([obs[agent] for agent in self._env.agents])\n",
    "#         # hands = state.player_hands.reshape((self._env.num_agents, -1))\n",
    "#         return all_obs\n",
    "        \n",
    "    \n",
    "#     def world_state_size(self):\n",
    "   \n",
    "#         return 18 # NOTE hardcoded hand size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ScopeParamShapeError",
     "evalue": "Initializer expected to generate shape (18, 128) but got shape (249, 128) instead for parameter \"kernel\" in \"/Dense_0\". (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeParamShapeError)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mScopeParamShapeError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 425\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;66;03m# Remove Hydra imports and decorators\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 425\u001b[0m     test \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 419\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m jax\u001b[38;5;241m.\u001b[39mdisable_jit(\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    418\u001b[0m     train_jit \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mjit(make_train(config))\n\u001b[0;32m--> 419\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_jit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrng\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[6], line 392\u001b[0m, in \u001b[0;36mmake_train.<locals>.train\u001b[0;34m(rng)\u001b[0m\n\u001b[1;32m    383\u001b[0m rng, _rng \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(rng)\n\u001b[1;32m    384\u001b[0m runner_state \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    385\u001b[0m     (actor_train_state, critic_train_state),\n\u001b[1;32m    386\u001b[0m     env_state,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m     _rng,\n\u001b[1;32m    391\u001b[0m )\n\u001b[0;32m--> 392\u001b[0m runner_state, metric \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_update_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mrunner_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNUM_UPDATES\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrunner_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: runner_state}\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[6], line 177\u001b[0m, in \u001b[0;36mmake_train.<locals>.train.<locals>._update_step\u001b[0;34m(update_runner_state, unused)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m runner_state, transition\n\u001b[1;32m    176\u001b[0m initial_hstates \u001b[38;5;241m=\u001b[39m runner_state[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m--> 177\u001b[0m runner_state, traj_batch \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_env_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunner_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNUM_STEPS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# CALCULATE ADVANTAGE\u001b[39;00m\n\u001b[1;32m    182\u001b[0m train_states, env_state, last_obs, last_done, hstates, rng \u001b[38;5;241m=\u001b[39m runner_state\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[6], line 138\u001b[0m, in \u001b[0;36mmake_train.<locals>.train.<locals>._update_step.<locals>._env_step\u001b[0;34m(runner_state, unused)\u001b[0m\n\u001b[1;32m    132\u001b[0m obs_batch \u001b[38;5;241m=\u001b[39m batchify(last_obs, env\u001b[38;5;241m.\u001b[39magents, config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNUM_ACTORS\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    133\u001b[0m ac_in \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    134\u001b[0m     obs_batch[np\u001b[38;5;241m.\u001b[39mnewaxis, :],\n\u001b[1;32m    135\u001b[0m     last_done[np\u001b[38;5;241m.\u001b[39mnewaxis, :],\n\u001b[1;32m    136\u001b[0m     avail_actions,\n\u001b[1;32m    137\u001b[0m )\n\u001b[0;32m--> 138\u001b[0m ac_hstate, pi \u001b[38;5;241m=\u001b[39m \u001b[43mactor_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_states\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhstates\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mac_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m action \u001b[38;5;241m=\u001b[39m pi\u001b[38;5;241m.\u001b[39msample(seed\u001b[38;5;241m=\u001b[39m_rng)\n\u001b[1;32m    140\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m pi\u001b[38;5;241m.\u001b[39mlog_prob(action)\n",
      "    \u001b[0;31m[... skipping hidden 6 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[4], line 47\u001b[0m, in \u001b[0;36mActorRNN.__call__\u001b[0;34m(self, hidden, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;129m@nn\u001b[39m\u001b[38;5;241m.\u001b[39mcompact\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden, x):\n\u001b[1;32m     46\u001b[0m     obs, dones, avail_actions \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m---> 47\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morthogonal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mrelu(embedding)\n\u001b[1;32m     52\u001b[0m     rnn_in \u001b[38;5;241m=\u001b[39m (embedding, dones)\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m/mnt/c/Users/jvnhe/Projects/RL-Roguelike-JAX-MARL/lib/python3.10/site-packages/flax/linen/linear.py:235\u001b[0m, in \u001b[0;36mDense.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;129m@compact\u001b[39m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Array) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Array:\n\u001b[1;32m    227\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Applies a linear transformation to the inputs along the last dimension.\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m    The transformed input.\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 235\u001b[0m   kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkernel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m      \u001b[49m\u001b[43m(\u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_bias:\n\u001b[1;32m    242\u001b[0m     bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam(\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_init, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures,), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_dtype\n\u001b[1;32m    244\u001b[0m     )\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/mnt/c/Users/jvnhe/Projects/RL-Roguelike-JAX-MARL/lib/python3.10/site-packages/flax/core/scope.py:971\u001b[0m, in \u001b[0;36mScope.param\u001b[0;34m(self, name, init_fn, unbox, *init_args)\u001b[0m\n\u001b[1;32m    966\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m val, abs_val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(value_flat, abs_value_flat):\n\u001b[1;32m    967\u001b[0m     \u001b[38;5;66;03m# NOTE: We could check dtype consistency here as well but it's\u001b[39;00m\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;66;03m# usefuleness is less obvious. We might intentionally change the dtype\u001b[39;00m\n\u001b[1;32m    969\u001b[0m     \u001b[38;5;66;03m# for inference to a half float type for example.\u001b[39;00m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mshape(val) \u001b[38;5;241m!=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mshape(abs_val):\n\u001b[0;32m--> 971\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mScopeParamShapeError(\n\u001b[1;32m    972\u001b[0m           name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_text, jnp\u001b[38;5;241m.\u001b[39mshape(abs_val), jnp\u001b[38;5;241m.\u001b[39mshape(val)\n\u001b[1;32m    973\u001b[0m       )\n\u001b[1;32m    974\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    975\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_mutable_collection(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;31mScopeParamShapeError\u001b[0m: Initializer expected to generate shape (18, 128) but got shape (249, 128) instead for parameter \"kernel\" in \"/Dense_0\". (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeParamShapeError)"
     ]
    }
   ],
   "source": [
    "# import jax\n",
    "# import jax.numpy as jnp\n",
    "# import numpy as np\n",
    "# import optax\n",
    "# from typing import Sequence, NamedTuple, Any, Tuple, Union, Dict\n",
    "\n",
    "# from flax.training.train_state import TrainState\n",
    "# import hydra\n",
    "# from omegaconf import DictConfig, OmegaConf\n",
    "# import jaxmarl\n",
    "# from jaxmarl.wrappers.baselines import LogWrapper\n",
    "\n",
    "# import wandb\n",
    "# from config import train_config\n",
    "\n",
    "# class Transition(NamedTuple):\n",
    "#     global_done: jnp.ndarray\n",
    "#     done: jnp.ndarray\n",
    "#     action: jnp.ndarray\n",
    "#     value: jnp.ndarray\n",
    "#     reward: jnp.ndarray\n",
    "#     log_prob: jnp.ndarray\n",
    "#     obs: jnp.ndarray\n",
    "#     world_state: jnp.ndarray\n",
    "#     info: jnp.ndarray\n",
    "#     avail_actions: jnp.ndarray\n",
    "\n",
    "\n",
    "# def batchify(x: dict, agent_list, num_actors):\n",
    "#     x = jnp.stack([x[a] for a in agent_list])\n",
    "#     return x.reshape((num_actors, -1))\n",
    "\n",
    "\n",
    "# def unbatchify(x: jnp.ndarray, agent_list, num_envs, num_actors):\n",
    "#     x = x.reshape((num_actors, num_envs, -1))\n",
    "#     return {a: x[i] for i, a in enumerate(agent_list)}\n",
    "\n",
    "\n",
    "# def make_train(config):\n",
    "#     env = environment_MARL.RL_Roguelike_JAX_MARL()\n",
    "#     config[\"NUM_ACTORS\"] = env.num_agents * config[\"NUM_ENVS\"]\n",
    "#     config[\"NUM_UPDATES\"] = (\n",
    "#         config[\"TOTAL_TIMESTEPS\"] // config[\"NUM_STEPS\"] // config[\"NUM_ENVS\"]\n",
    "#     )\n",
    "#     config[\"MINIBATCH_SIZE\"] = (\n",
    "#         config[\"NUM_ACTORS\"] * config[\"NUM_STEPS\"] // config[\"NUM_MINIBATCHES\"]\n",
    "#     )\n",
    "#     config[\"CLIP_EPS\"] = config[\"CLIP_EPS\"] / env.num_agents if config[\"SCALE_CLIP_EPS\"] else config[\"CLIP_EPS\"]\n",
    "\n",
    "#     # env = FlattenObservationWrapper(env) # NOTE need a batchify wrapper\n",
    "#     env = HanabiWorldStateWrapper(env)\n",
    "#     env = LogWrapper(env)\n",
    "\n",
    "#     def linear_schedule(count):\n",
    "#         frac = (\n",
    "#             1.0\n",
    "#             - (count // (config[\"NUM_MINIBATCHES\"] * config[\"UPDATE_EPOCHS\"]))\n",
    "#             / config[\"NUM_UPDATES\"]\n",
    "#         )\n",
    "#         return config[\"LR\"] * frac\n",
    "\n",
    "#     def train(rng):\n",
    "#         # INIT NETWORK\n",
    "#         actor_network = ActorRNN(11, config=config)\n",
    "#         critic_network = CriticRNN()\n",
    "#         rng, _rng_actor, _rng_critic = jax.random.split(rng, 3)\n",
    "#         ac_init_x = (\n",
    "#             jnp.zeros((1, config[\"NUM_ENVS\"], 18)),\n",
    "#             jnp.zeros((1, config[\"NUM_ENVS\"])),\n",
    "#             jnp.zeros((1, config[\"NUM_ENVS\"], 11)),\n",
    "#         )\n",
    "#         ac_init_hstate = ScannedRNN.initialize_carry(config[\"NUM_ENVS\"], 128)\n",
    "#         actor_network_params = actor_network.init(_rng_actor, ac_init_hstate, ac_init_x)\n",
    "        \n",
    "#         cr_init_x = (\n",
    "#             jnp.zeros((1, config[\"NUM_ENVS\"], env.world_state_size(),)), \n",
    "#             jnp.zeros((1, config[\"NUM_ENVS\"])),\n",
    "#         )\n",
    "#         cr_init_hstate = ScannedRNN.initialize_carry(config[\"NUM_ENVS\"], 128)\n",
    "#         critic_network_params = critic_network.init(_rng_critic, cr_init_hstate, cr_init_x)\n",
    "        \n",
    "#         if config[\"ANNEAL_LR\"]:\n",
    "#             actor_tx = optax.chain(\n",
    "#                 optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "#                 optax.adam(learning_rate=linear_schedule, eps=1e-5),\n",
    "#             )\n",
    "#             critic_tx = optax.chain(\n",
    "#                 optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "#                 optax.adam(learning_rate=linear_schedule, eps=1e-5),\n",
    "#             )\n",
    "#         else:\n",
    "#             actor_tx = optax.chain(\n",
    "#                 optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "#                 optax.adam(config[\"LR\"], eps=1e-5),\n",
    "#             )\n",
    "#             critic_tx = optax.chain(\n",
    "#                 optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "#                 optax.adam(config[\"LR\"], eps=1e-5),\n",
    "#             )\n",
    "#         actor_train_state = TrainState.create(\n",
    "#             apply_fn=actor_network.apply,\n",
    "#             params=actor_network_params,\n",
    "#             tx=actor_tx,\n",
    "#         )\n",
    "#         critic_train_state = TrainState.create(\n",
    "#             apply_fn=actor_network.apply,\n",
    "#             params=critic_network_params,\n",
    "#             tx=critic_tx,\n",
    "#         )\n",
    "\n",
    "#         # INIT ENV\n",
    "#         rng, _rng = jax.random.split(rng)\n",
    "#         reset_rng = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "#         obsv, env_state = jax.vmap(env.reset, in_axes=(0,))(reset_rng)\n",
    "#         ac_init_hstate = ScannedRNN.initialize_carry(config[\"NUM_ACTORS\"], 128)\n",
    "#         cr_init_hstate = ScannedRNN.initialize_carry(config[\"NUM_ACTORS\"], 128)\n",
    "\n",
    "#         # TRAIN LOOP\n",
    "#         def _update_step(update_runner_state, unused):\n",
    "#             # COLLECT TRAJECTORIES\n",
    "#             runner_state, update_steps = update_runner_state\n",
    "            \n",
    "#             def _env_step(runner_state, unused):\n",
    "#                 train_states, env_state, last_obs, last_done, hstates, rng = runner_state\n",
    "\n",
    "#                 # SELECT ACTION\n",
    "#                 rng, _rng = jax.random.split(rng)\n",
    "#                 avail_actions = jax.vmap(env.get_legal_moves)(env_state.env_state)\n",
    "#                 avail_actions = jax.lax.stop_gradient(\n",
    "#                     batchify(avail_actions, env.agents, config[\"NUM_ACTORS\"])\n",
    "#                 )\n",
    "#                 obs_batch = batchify(last_obs, env.agents, config[\"NUM_ACTORS\"])\n",
    "#                 ac_in = (\n",
    "#                     obs_batch[np.newaxis, :],\n",
    "#                     last_done[np.newaxis, :],\n",
    "#                     avail_actions,\n",
    "#                 )\n",
    "#                 ac_hstate, pi = actor_network.apply(train_states[0].params, hstates[0], ac_in)\n",
    "#                 action = pi.sample(seed=_rng)\n",
    "#                 log_prob = pi.log_prob(action)\n",
    "#                 env_act = unbatchify(\n",
    "#                     action, env.agents, config[\"NUM_ENVS\"], env.num_agents\n",
    "#                 )\n",
    "\n",
    "#                 # VALUE\n",
    "#                 world_state = last_obs[\"world_state\"].reshape((config[\"NUM_ACTORS\"],-1))\n",
    "#                 cr_in = (\n",
    "#                     world_state[None, :],\n",
    "#                     last_done[np.newaxis, :],\n",
    "#                 )\n",
    "#                 cr_hstate, value = critic_network.apply(train_states[1].params, hstates[1], cr_in)\n",
    "\n",
    "#                 # STEP ENV\n",
    "#                 rng, _rng = jax.random.split(rng)\n",
    "#                 rng_step = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "#                 obsv, env_state, reward, done, info = jax.vmap(\n",
    "#                     env.step, in_axes=(0, 0, 0)\n",
    "#                 )(rng_step, env_state, env_act)\n",
    "#                 info = jax.tree_map(lambda x: x.reshape((config[\"NUM_ACTORS\"])), info)\n",
    "#                 done_batch = batchify(done, env.agents, config[\"NUM_ACTORS\"]).squeeze()\n",
    "#                 transition = Transition(\n",
    "#                     jnp.tile(done[\"__all__\"], env.num_agents),\n",
    "#                     done_batch,\n",
    "#                     action.squeeze(),\n",
    "#                     value.squeeze(),\n",
    "#                     batchify(reward, env.agents, config[\"NUM_ACTORS\"]).squeeze(),\n",
    "#                     log_prob.squeeze(),\n",
    "#                     obs_batch,\n",
    "#                     world_state,\n",
    "#                     info,\n",
    "#                     avail_actions,\n",
    "#                 )\n",
    "#                 runner_state = (train_states, env_state, obsv, done_batch, (ac_hstate, cr_hstate), rng)\n",
    "#                 return runner_state, transition\n",
    "\n",
    "#             initial_hstates = runner_state[-2]\n",
    "#             runner_state, traj_batch = jax.lax.scan(\n",
    "#                 _env_step, runner_state, None, config[\"NUM_STEPS\"]\n",
    "#             )\n",
    "            \n",
    "#             # CALCULATE ADVANTAGE\n",
    "#             train_states, env_state, last_obs, last_done, hstates, rng = runner_state\n",
    "      \n",
    "#             last_world_state = last_obs[\"world_state\"].reshape((config[\"NUM_ACTORS\"],-1))\n",
    "#             cr_in = (\n",
    "#                 last_world_state[None, :],\n",
    "#                 last_done[np.newaxis, :],\n",
    "#             )\n",
    "#             _, last_val = critic_network.apply(train_states[1].params, hstates[1], cr_in)\n",
    "#             last_val = last_val.squeeze()\n",
    "\n",
    "#             def _calculate_gae(traj_batch, last_val):\n",
    "#                 def _get_advantages(gae_and_next_value, transition):\n",
    "#                     gae, next_value = gae_and_next_value\n",
    "#                     done, value, reward = (\n",
    "#                         transition.global_done,\n",
    "#                         transition.value,\n",
    "#                         transition.reward,\n",
    "#                     )\n",
    "#                     delta = reward + config[\"GAMMA\"] * next_value * (1 - done) - value\n",
    "#                     gae = (\n",
    "#                         delta\n",
    "#                         + config[\"GAMMA\"] * config[\"GAE_LAMBDA\"] * (1 - done) * gae\n",
    "#                     )\n",
    "#                     return (gae, value), gae\n",
    "\n",
    "#                 _, advantages = jax.lax.scan(\n",
    "#                     _get_advantages,\n",
    "#                     (jnp.zeros_like(last_val), last_val),\n",
    "#                     traj_batch,\n",
    "#                     reverse=True,\n",
    "#                     unroll=16,\n",
    "#                 )\n",
    "#                 return advantages, advantages + traj_batch.value\n",
    "\n",
    "#             advantages, targets = _calculate_gae(traj_batch, last_val)\n",
    "\n",
    "#             # UPDATE NETWORK\n",
    "#             def _update_epoch(update_state, unused):\n",
    "#                 def _update_minbatch(train_states, batch_info):\n",
    "#                     actor_train_state, critic_train_state = train_states\n",
    "#                     ac_init_hstate, cr_init_hstate, traj_batch, advantages, targets = batch_info\n",
    "\n",
    "#                     def _actor_loss_fn(actor_params, init_hstate, traj_batch, gae):\n",
    "#                         # RERUN NETWORK\n",
    "#                         _, pi = actor_network.apply(\n",
    "#                             actor_params,\n",
    "#                             init_hstate.transpose(),\n",
    "#                             (traj_batch.obs, traj_batch.done, traj_batch.avail_actions),\n",
    "#                         )\n",
    "#                         log_prob = pi.log_prob(traj_batch.action)\n",
    "\n",
    "#                         # CALCULATE ACTOR LOSS\n",
    "#                         ratio = jnp.exp(log_prob - traj_batch.log_prob)\n",
    "#                         gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n",
    "#                         loss_actor1 = ratio * gae\n",
    "#                         loss_actor2 = (\n",
    "#                             jnp.clip(\n",
    "#                                 ratio,\n",
    "#                                 1.0 - config[\"CLIP_EPS\"],\n",
    "#                                 1.0 + config[\"CLIP_EPS\"],\n",
    "#                             )\n",
    "#                             * gae\n",
    "#                         )\n",
    "#                         loss_actor = -jnp.minimum(loss_actor1, loss_actor2)\n",
    "#                         loss_actor = loss_actor.mean(where=(1 - traj_batch.done))\n",
    "#                         entropy = pi.entropy().mean(where=(1 - traj_batch.done))\n",
    "#                         actor_loss = (\n",
    "#                             loss_actor\n",
    "#                             - config[\"ENT_COEF\"] * entropy\n",
    "#                         )\n",
    "#                         return actor_loss, (loss_actor, entropy)\n",
    "                    \n",
    "#                     def _critic_loss_fn(critic_params, init_hstate, traj_batch, targets):\n",
    "#                         # RERUN NETWORK\n",
    "#                         _, value = critic_network.apply(critic_params, init_hstate.transpose(), (traj_batch.world_state,  traj_batch.done)) \n",
    "                        \n",
    "#                         # CALCULATE VALUE LOSS\n",
    "#                         value_pred_clipped = traj_batch.value + (\n",
    "#                             value - traj_batch.value\n",
    "#                         ).clip(-config[\"CLIP_EPS\"], config[\"CLIP_EPS\"])\n",
    "#                         value_losses = jnp.square(value - targets)\n",
    "#                         value_losses_clipped = jnp.square(value_pred_clipped - targets)\n",
    "#                         value_loss = (\n",
    "#                             0.5 * jnp.maximum(value_losses, value_losses_clipped).mean(where=(1 - traj_batch.done))\n",
    "#                         )\n",
    "#                         critic_loss = config[\"VF_COEF\"] * value_loss\n",
    "#                         return critic_loss, (value_loss)\n",
    "\n",
    "#                     actor_grad_fn = jax.value_and_grad(_actor_loss_fn, has_aux=True)\n",
    "#                     actor_loss, actor_grads = actor_grad_fn(\n",
    "#                         actor_train_state.params, ac_init_hstate, traj_batch, advantages\n",
    "#                     )\n",
    "#                     critic_grad_fn = jax.value_and_grad(_critic_loss_fn, has_aux=True)\n",
    "#                     critic_loss, critic_grads = critic_grad_fn(\n",
    "#                         critic_train_state.params, cr_init_hstate, traj_batch, targets\n",
    "#                     )\n",
    "                    \n",
    "#                     actor_train_state = actor_train_state.apply_gradients(grads=actor_grads)\n",
    "#                     critic_train_state = critic_train_state.apply_gradients(grads=critic_grads)\n",
    "                    \n",
    "#                     total_loss = actor_loss[0] + critic_loss[0]\n",
    "#                     loss_info = {\n",
    "#                         \"total_loss\": total_loss,\n",
    "#                         \"actor_loss\": actor_loss[0],\n",
    "#                         \"critic_loss\": critic_loss[0],\n",
    "#                         \"entropy\": actor_loss[1][1],\n",
    "#                     }\n",
    "                    \n",
    "#                     return (actor_train_state, critic_train_state), loss_info\n",
    "\n",
    "#                 (\n",
    "#                     train_states,\n",
    "#                     init_hstates,\n",
    "#                     traj_batch,\n",
    "#                     advantages,\n",
    "#                     targets,\n",
    "#                     rng,\n",
    "#                 ) = update_state\n",
    "#                 rng, _rng = jax.random.split(rng)\n",
    "\n",
    "#                 init_hstates = jax.tree_map(lambda x: jnp.reshape(\n",
    "#                     x, (config[\"NUM_STEPS\"], config[\"NUM_ACTORS\"])\n",
    "#                 ), init_hstates)\n",
    "                \n",
    "#                 batch = (\n",
    "#                     init_hstates[0],\n",
    "#                     init_hstates[1],\n",
    "#                     traj_batch,\n",
    "#                     advantages.squeeze(),\n",
    "#                     targets.squeeze(),\n",
    "#                 )\n",
    "#                 permutation = jax.random.permutation(_rng, config[\"NUM_ACTORS\"])\n",
    "\n",
    "#                 shuffled_batch = jax.tree_util.tree_map(\n",
    "#                     lambda x: jnp.take(x, permutation, axis=1), batch\n",
    "#                 )\n",
    "\n",
    "#                 minibatches = jax.tree_util.tree_map(\n",
    "#                     lambda x: jnp.swapaxes(\n",
    "#                         jnp.reshape(\n",
    "#                             x,\n",
    "#                             [x.shape[0], config[\"NUM_MINIBATCHES\"], -1]\n",
    "#                             + list(x.shape[2:]),\n",
    "#                         ),\n",
    "#                         1,\n",
    "#                         0,\n",
    "#                     ),\n",
    "#                     shuffled_batch,\n",
    "#                 )\n",
    "\n",
    "#                 train_states, loss_info = jax.lax.scan(\n",
    "#                     _update_minbatch, train_states, minibatches\n",
    "#                 )\n",
    "#                 update_state = (\n",
    "#                     train_states,\n",
    "#                     init_hstates,\n",
    "#                     traj_batch,\n",
    "#                     advantages,\n",
    "#                     targets,\n",
    "#                     rng,\n",
    "#                 )\n",
    "#                 return update_state, loss_info\n",
    "\n",
    "#             ac_init_hstate = initial_hstates[0][None, :].squeeze().transpose()\n",
    "#             cr_init_hstate = initial_hstates[1][None, :].squeeze().transpose()\n",
    "\n",
    "#             update_state = (\n",
    "#                 train_states,\n",
    "#                 (ac_init_hstate, cr_init_hstate),\n",
    "#                 traj_batch,\n",
    "#                 advantages,\n",
    "#                 targets,\n",
    "#                 rng,\n",
    "#             )\n",
    "#             update_state, loss_info = jax.lax.scan(\n",
    "#                 _update_epoch, update_state, None, config[\"UPDATE_EPOCHS\"]\n",
    "#             )\n",
    "#             loss_info = jax.tree_map(lambda x: x.mean(), loss_info)\n",
    "            \n",
    "#             train_states = update_state[0]\n",
    "#             metric = traj_batch.info\n",
    "#             rng = update_state[-1]\n",
    "\n",
    "#             # def callback(metric):\n",
    "                \n",
    "#             #     wandb.log(\n",
    "#             #         {\n",
    "#             #             \"returns\": metric[\"returned_episode_returns\"][-1, :].mean(),\n",
    "#             #             \"env_step\": metric[\"update_steps\"]\n",
    "#             #             * config[\"NUM_ENVS\"]\n",
    "#             #             * config[\"NUM_STEPS\"],\n",
    "#             #         }\n",
    "#             #     )\n",
    "                \n",
    "            \n",
    "#             # metric[\"update_steps\"] = update_steps\n",
    "#             # jax.experimental.io_callback(callback, None, metric)\n",
    "#             update_steps = update_steps + 1\n",
    "#             runner_state = (train_states, env_state, last_obs, last_done, hstates, rng)\n",
    "#             return (runner_state, update_steps), metric\n",
    "\n",
    "#         rng, _rng = jax.random.split(rng)\n",
    "#         runner_state = (\n",
    "#             (actor_train_state, critic_train_state),\n",
    "#             env_state,\n",
    "#             obsv,\n",
    "#             jnp.zeros((config[\"NUM_ACTORS\"]), dtype=bool),\n",
    "#             (ac_init_hstate, cr_init_hstate),\n",
    "#             _rng,\n",
    "#         )\n",
    "#         runner_state, metric = jax.lax.scan(\n",
    "#             _update_step, (runner_state, 0), None, config[\"NUM_UPDATES\"]\n",
    "#         )\n",
    "#         return {\"runner_state\": runner_state}\n",
    "\n",
    "#     return train\n",
    "\n",
    "# def main():\n",
    "#     \"\"\"Main training function using direct config.\"\"\"\n",
    "#     # Use the train_config directly from config.py\n",
    "#     config = train_config\n",
    "    \n",
    "#     # Initialize wandb\n",
    "#     wandb.init(\n",
    "#         entity=config[\"ENTITY\"],\n",
    "#         project=config[\"PROJECT\"],\n",
    "#         tags=[\"MAPPO\", \"RNN\", config[\"ENV_NAME\"]],\n",
    "#         config=config,\n",
    "#         mode=config[\"WANDB_MODE\"],\n",
    "#     )\n",
    "    \n",
    "#     # Set random seed\n",
    "#     rng = jax.random.PRNGKey(config[\"SEED\"])\n",
    "    \n",
    "#     # Run training\n",
    "#     with jax.disable_jit(False):\n",
    "#         train_jit = jax.jit(make_train(config))\n",
    "#         out = train_jit(rng)\n",
    "\n",
    "#     return out\n",
    "\n",
    "# # Remove Hydra imports and decorators\n",
    "# if __name__ == \"__main__\":\n",
    "#     test = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Get action by ID\\naction_info = action_registry.get_by_id(0)\\nif action_info:\\n    action = action_info.create_fn()\\n\\n# Get action by name \\naction_info = action_registry.get_by_name(\"SuicideAction\")\\nif action_info:\\n    action = action_info.create_fn()\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class ActionInfo:\n",
    "#     def __init__(self, id: int, name: str, create_fn):\n",
    "#         self.id = id\n",
    "#         self.name = name \n",
    "#         self.create_fn = create_fn\n",
    "\n",
    "# class ActionRegistry:\n",
    "#     def __init__(self):\n",
    "#         self.actions = {}  # name -> ActionInfo\n",
    "#         self.id_lookup = {} # id -> ActionInfo\n",
    "#         self._next_id = 0\n",
    "    \n",
    "#     def register(self, name: str, create_fn) -> None:\n",
    "#         action_id = self._next_id\n",
    "#         info = ActionInfo(action_id, name, create_fn)\n",
    "#         self.actions[name] = info\n",
    "#         self.id_lookup[action_id] = info\n",
    "#         self._next_id += 1\n",
    "        \n",
    "#     def get_by_name(self, name: str) -> ActionInfo:\n",
    "#         return self.actions.get(name)\n",
    "        \n",
    "#     def get_by_id(self, id: int) -> ActionInfo:\n",
    "#         return self.id_lookup.get(id)\n",
    "\n",
    "# # Replace global registry\n",
    "# action_registry = ActionRegistry()\n",
    "\n",
    "# # Update register_action function\n",
    "# def register_action(name: str, create_fn):\n",
    "#     action_registry.register(name, create_fn)\n",
    "\n",
    "# # Example usage:\n",
    "# \"\"\"\n",
    "# # Get action by ID\n",
    "# action_info = action_registry.get_by_id(0)\n",
    "# if action_info:\n",
    "#     action = action_info.create_fn()\n",
    "\n",
    "# # Get action by name \n",
    "# action_info = action_registry.get_by_name(\"SuicideAction\")\n",
    "# if action_info:\n",
    "#     action = action_info.create_fn()\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ability_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ability_registry = ability_actions.ability_registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ability_registry.num_abilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import jax.numpy as jnp\n",
    "# from jax import lax, debug\n",
    "# from utils import euclidean_distance, is_within_bounds, is_collision, do_invalid_move, do_damage\n",
    "# from actions import Action\n",
    "# from data_classes import DamageType\n",
    "\n",
    "# class AbilityRegistry:\n",
    "#     def __init__(self):\n",
    "#         self.abilities = []  # List of (name, create_fn) tuples\n",
    "#         self.num_abilities = 0\n",
    "\n",
    "#     def register(self, name, create_fn):\n",
    "#         \"\"\"Register ability with auto-incrementing index\"\"\"\n",
    "#         self.abilities.append((name, create_fn))\n",
    "#         self.num_abilities += 1\n",
    "#         return self.num_abilities - 1  # Return index of registered ability\n",
    "\n",
    "#     def get_by_index(self, index):\n",
    "#         \"\"\"Get ability create_fn by index\"\"\"\n",
    "#         return self.abilities[index][1]\n",
    "\n",
    "# # Create global registry\n",
    "# ability_registry = AbilityRegistry()\n",
    "\n",
    "# # suicide:\n",
    "# # Register suicide action\n",
    "# SUICIDE_ACTION_IDX = ability_registry.register(\"SuicideAction\", lambda: SuicideAction())\n",
    "\n",
    "# class SuicideAction(Action):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self._base_cooldown = jnp.int32(3)  # Set cooldown to 3\n",
    "#         self.ability_index = SUICIDE_ACTION_IDX\n",
    "#         self.base_damage = jnp.float32(5.0)\n",
    "#         self.range = jnp.float32(8.0)\n",
    "#         self._ability_description = \"Deal damage based on strength to an enemy and take damage yourself\"\n",
    "        \n",
    "#     def is_valid(self, state, unit, target):\n",
    "#         enough_action_points = unit.action_points_current >= 1\n",
    "#         within_range = state.distance_to_enemy <= self.range\n",
    "#         return jnp.logical_and(enough_action_points, within_range)\n",
    "\n",
    "#     def _perform_action(self, state, unit, target):\n",
    "#         # Generate all 8 adjacent grid positions\n",
    "#         adjacent_positions = [\n",
    "#             (target.location_x + dx, target.location_y + dy)\n",
    "#             for dx, dy in [(-1,-1), (-1,0), (-1,1), (0,-1), (0,1), (1,-1), (1,0), (1,1)]\n",
    "#         ]\n",
    "        \n",
    "#         # Find closest valid adjacent position\n",
    "#         best_x, best_y = adjacent_positions[0]\n",
    "#         best_dist = euclidean_distance(unit.location_x, unit.location_y, best_x, best_y)\n",
    "        \n",
    "#         def update_best_position(i, val):\n",
    "#             x, y = adjacent_positions[i]\n",
    "#             dist = euclidean_distance(unit.location_x, unit.location_y, x, y)\n",
    "#             use_new = jnp.logical_and(\n",
    "#                 is_within_bounds(x, y),\n",
    "#                 dist < val[2]\n",
    "#             )\n",
    "#             return lax.cond(\n",
    "#                 use_new,\n",
    "#                 lambda _: (x, y, dist),\n",
    "#                 lambda _: val,\n",
    "#                 None\n",
    "#             )\n",
    "            \n",
    "#         new_x, new_y, _ = lax.fori_loop(1, 8, update_best_position, (best_x, best_y, best_dist))\n",
    "\n",
    "#         damage_dealt = self.base_damage + unit.strength_current\n",
    "\n",
    "#         # Apply damage using do_damage utility\n",
    "#         new_unit, new_target = do_damage(unit, target, damage_dealt, DamageType.PURE)\n",
    "#         # Do base_damage to self\n",
    "#         new_unit, _ = do_damage(unit, new_unit, self.base_damage, DamageType.PURE) #TODO: don't return self damage\n",
    "        \n",
    "#         # Update position and action points\n",
    "#         new_unit = new_unit.replace(\n",
    "#             action_points_current=jnp.float32(new_unit.action_points_current - 1),\n",
    "#             location_x=jnp.float32(new_x),\n",
    "#             location_y=jnp.float32(new_y)\n",
    "#         )\n",
    "        \n",
    "#         # Calculate new distance\n",
    "#         new_distance = euclidean_distance(new_x, new_y, target.location_x, target.location_y)\n",
    "        \n",
    "#         return lax.cond(\n",
    "#             jnp.equal(state.player.unit_id, unit.unit_id),\n",
    "#             lambda: state.replace(\n",
    "#                 player=new_unit, \n",
    "#                 enemy=new_target,\n",
    "#                 distance_to_enemy=new_distance\n",
    "#             ),\n",
    "#             lambda: state.replace(\n",
    "#                 player=new_target, \n",
    "#                 enemy=new_unit,\n",
    "#                 distance_to_enemy=new_distance\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "# # Steal Strength - reduce enemy strength and increase own strength\n",
    "# # Register the new action\n",
    "# STEAL_STRENGTH_IDX = ability_registry.register(\"StealStrengthAction\", lambda: StealStrengthAction())\n",
    "\n",
    "# class StealStrengthAction(Action):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.ability_index = STEAL_STRENGTH_IDX\n",
    "#         self._base_cooldown = jnp.int32(1)\n",
    "#         self.range = jnp.float32(4.0)\n",
    "#         self.strength_steal_amount = jnp.float32(2.0)\n",
    "#         self._ability_description = \"Steal 2 strength from the target\"\n",
    "\n",
    "#     def is_valid(self, state, unit, target):\n",
    "#         enough_action_points = unit.action_points_current >= 1\n",
    "#         within_range = state.distance_to_enemy <= self.range\n",
    "#         return jnp.logical_and(enough_action_points, within_range)\n",
    "\n",
    "#     def _perform_action(self, state, unit, target):\n",
    "#         # Reduce target's strength\n",
    "#         new_target = target.replace(\n",
    "#             strength_current=jnp.maximum(0, target.strength_current - self.strength_steal_amount)\n",
    "#         )\n",
    "        \n",
    "#         # Increase caster's strength and reduce action points\n",
    "#         new_unit = unit.replace(\n",
    "#             strength_current=unit.strength_current + self.strength_steal_amount,\n",
    "#             action_points_current=unit.action_points_current - 1\n",
    "#         )\n",
    "\n",
    "#         return lax.cond(\n",
    "#             jnp.equal(state.player.unit_id, unit.unit_id),\n",
    "#             lambda: state.replace(player=new_unit, enemy=new_target),\n",
    "#             lambda: state.replace(player=new_target, enemy=new_unit)\n",
    "#         )\n",
    "    \n",
    "# # Strength Regen - regen health based on your strength\n",
    "# # Add barrier - add a barrier based on resolve\n",
    "# # Mana Burn\n",
    "# # Multi Attack\n",
    "# # Return\n",
    "# # Fury Swipes\n",
    "# # Push\n",
    "# # Frost Arrows\n",
    "# # Stun\n",
    "# # Hook\n",
    "# # Lifesteal / feast\n",
    "# # Int steal\n",
    "# # Int based nuke\n",
    "# # Armour reduction\n",
    "# # Add barrier\n",
    "# # Spellsteal\n",
    "# # Fracture casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL-Roguelike-JAX-MARL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
